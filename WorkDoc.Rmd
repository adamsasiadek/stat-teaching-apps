---
title: "A Gentle but Critical Introduction to Statistical Inference"
author: "Wouter de Nooy"
date: "2016-09-20 - `r Sys.Date()`"
runtime: shiny
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=12, echo=FALSE, warning=FALSE, message=FALSE, eval=TRUE, tidy=TRUE)
#currently: 26 interactive content items
```

## Introduction And Reader's Guide

In the years that I have been teaching inferential statistics to bachelor students in Communication Science, I have learned two things. First, it is paramount that students understand the principles of statistical inference very well before they can apply statistical inference correctly themselves. Second, formal notation, manual calculation, and estimation details distract this type of students rather than help them understand what they are doing. This book, then, offers a non-technical but thorough introduction to statistical inference. It discusses a minimal set of concepts needed to understand both the possibilities and pitfalls of estimation and null hypothesis testing. It uses a minimum of formal notation.

#### Intended audience and setting

This book is written as reading material for a second course in statistics in the bachelor Communication Science at the University of Amsterdam. Students enrolled in this course have had a first course in statistics that explained them how to change research questions into variables and associations between variables, how to select and execute (in SPSS) the right analysis or test to answer their research question, and how to interpret the results in a langage that is both understandable to the average reader and complying with professional standards (APA6 standard for reporting test results). In addition, they have learned the very basics of inferential statistics: how and which null hypothesis to reject based on reported p-values and how to interpret confidence intervals.  

This book is meant for use in a flipped classroom setup. Students should read the text, watch embedded videos, and play with the interactive content before they meet in class. Class meetings are used to answer questions raised by the students, do group work to exercise with the concepts and techniques presented in the text, and do little tests to check understanding.  

#### Interactive content

The interactive content in this book replaces simulations that otherwise would have been demonstrated during lectures or in video clips. I expect that doing simulations yourself rather than watching them being done by someone else enhances understanding. I have tried to break down the simulations into small steps, confronting the student several times with basically the same simulation but with some added complexity. I hope that this approach enhances understanding and remembrance and, at the same time, avoids frustration caused by complex dashboards offering all options at once. Most interactive content starts with a question regarding the student's expectations of what is goig to happen in the simulation. I strongly recommend students to state their expectations before they start the simulations to see where their intuitions are right and where they are wrong.  

__TBD: thank you's and sources used for text and interactive content__

## Chapter 1. Sampling Distribution: How Different Could My Sample Have Been?
> Key concepts: scope, population, random sample, generalization, representative sample, parameter, sample statistic, random variable, sampling distribution, expected value, probability, probability distribution, sampling space, discrete probability distribution, continuous probability distribution, p-value.  

Statistical inference is about estimation and null hypothesis testing. Sampling distributions are the central element in estimation and null hypothesis testing. In this chapter, we simulate sampling distributions to understand what they are. Here, _simulation_ means that we have a computer draw many random samples.

### 1.1 Statistical Inference: Making The Most Of Your Data

Statistics is a tool for scientific research. It offers a range of techniques to check whether statements about the observable world (empirical reality) are supported by data collected from that world. Scientific theories strive for general statements, that is, statements that apply to many situations. Checking these statements requires lots data covering all situations covered by theory.

But collecting data is expensive, so we would like to collect as little data as possible but still be able to draw conclusions about a much larger set. The costs and time involved in collecting large sets of data are also relevant to applied research, for example, market research. In this context we also like to collect as little data as necessary.  

_Inferential statistics_ offers techniques to make statements about a larger set of situations from data collected for a smaller set of situations. The large set of situations about which we want to make a statement is called the _population_. The smaller set is caled a _sample_ and we want to _generalize_ a statement about the sample to a statement about the population from which the sample was drawn.  

Traditionally, statistical inference is generalization from the data collected in a _random sample_ to the population from which the sample was drawn. This approach is the focus of the present book because it is currently the most widely used type of statistical inference in the social sciences. We will, however, point out other approaches in the concluding chapter.  

Statistical inference is conceptually complicated and for that reason quite often used incorrectly. We will therefore spend quite some time on the principles of statistical inference. Good understanding of the principles should help students recognize and avoid incorrect use of statistical inference. In addition, it should help the student to understand the controversies surrounding statistical inference and developments in the practice of applying statistical inference that are taking place. Investing time and energy in fully understanding the principles of statistical inference really pays off later.  

### 1.2 A Discrete Random Variable: How Many Yellow Candies In My Bag?

An obvious but key insight in statistical inference is this: If we draw random samples from the same population, we are likely to obtain different samples. No two random samples from the same population need to be identical even though they can be identical. 

#### Sample statistic  
We are usually interested in a particular characteristic of the sample rather than in the exact nature of each observation within the sample. For example, I am very fond of yellow candies. If I buy a bag of candies, my first impulse is to tear the bag open and count the number of yellow candies. Am I lucky today: Does my bag contain many yellow candies?

The interactive content below shows a population of candies. The button allows you to draw a random sample of ten candies from this population. Before you press the button, answer the question: What do you expect about the number of yellow candies in the sample? Then, draw four samples and check whether your expectations come true.

```{r random_variable}
#interactive content: draw a sample (up to four samples) from a population of points uniformly distributed over five colors, display the sample (as set of colored circles) and the number of yellow candies with each sample ; relevant expectations: (1) number of yellow candies per sample varies (variable), (2) this number depends on chance (random variable), (3) the number may rangr from 0 to 10 (sampling space), (4) the most likely number of yellow candies is two (expected value, expectation).
#relates to inetractive content 'sampling_distribution'
```

The number of yellow candies in a bag is an example of a _sample statistic_: a number describing a property of the sample. Each bag, that is, each sample, has one score on the sample statistic. For example, one bag contains four yellow candies, another bag contains seven, and so on.

The sample statistic is called a _random variable_. It is a variable because it assigns a score to a sample and different samples can have different scores (variation). The value of a random variable may vary from sample to sample. It is a random variable because the score depends on chance, namely chance arising during random sampling.   

The collection of all possible outcome scores of a random variable are called _sampling space_. In the example of a (sample) bag containing twenty candies, the number of yellow candies can range from zero (I am very very unlucky) to twenty (I am super lucky).  The sampling space then contains the integer values from zero to twenty.

#### Sampling distribution  
Some outcomes occur more often than other outcomes. We can see this if we draw very many samples from a population and make a histogram of the frequencies at which outcomes occur. We call the distribution of the outcomes of very many samples a _sampling distribution_.  

Guess the most likely and most unlikely outcomes for the number of yellow candies in a sample bag containing ten candies. Then check your intuitions with the interactive content below by drawing samples from a population containing candies equally distributed over five colors.  

```{r sampling_distribution}
#interactive content: three histograms: a uniformly distributed discrete population of five colors on top, a sample in the middle, and a sampling distribution in the bottom ; first button allows to draw one sample, simulating drawing one sample from the population and adding the number of yellow candies to the bottom histogram (ideally, the candies 'drop' from the population to the sample), then the number of yellow candies appears below the sample and this number 'drops' from the sample to the sampling distribution ; second button (becomes active after the first button has been used) draws 1,000 samples and adds the yellow candy counts for all samples to the sampling distribution in one go
#simplified, discrete distribution version of https://ihstevenson.shinyapps.io/sample_means/ or https://github.com/DataScienceUWL/samplingShiny or https://github.com/jodeleeuw/shiny-stats/tree/master/populations-and-samples
```

In the interactive content:  

* What do the numbers on the horizontal axis of the histograms mean?  
* What are the observations in the three histograms? Hnt: There are two different types of observations.  

The sampling distribution tells us the probabilities of obtaining each outcome, for example the probability of buying a bag with exactly five yellow candies. To this end, we must change the (absolute) frequencies into proportions (relative frequencies). We have to divide the number of times an outcome occurs in the sampling distribution by the number of samples we have drawn. A sampling space with a probability (between 0 and 1) for each outcome is called a _probability distribution_. The table below shows the probability distribution of the number of yellow candies per bag of twenty candies. This is an example of a _discrete probability distribution_ because only a limited number of outcomes are possible, so it is feasible to list the probability of each outcome separately.

__add table; interactively change sample size, hence the set of outcomes and their probabilities?__

#### Probability and probability distribution
We may refer to probabilities both as a proportion, that is, a number between 0 and 1, and as a percentage: a number between 0% and 100%. Proportions are commonly supposed to be the right way to express probabilities. When we talk about probabilities, however, we tend to use percentages, for example, if we say that the probabilities are fifty-fifty.  

The sampling distribution conveys very important information. It tells us which outcomes we may expect, for example, how many yellow candies we may find in our bag. Moreover, it tells us the probability that a particular outcome may occur. For example, we are quite likely to find three, four, or five yellow candies in our bag but we are very unlikely to have a bag containing more than ten yellow candies; if we bought such a bag, we know that we have been very lucky.  

#### Expectation  
We haven't yet thought about the value that we are most likely to encounter in the sample that we are going to draw. Intuitively, it must be related to the distribution of colors in the population of candies from which the sample was drawn. In other words, the share of yellow candies in the huge container from which the bag was filled or in the machine that produces the candies, seems to be relevant to what we may expect to find in our sample.  

If the share of yellow candies in the population is 0.20 or, equivalently, 20%, we expect one out of each five candies in a bag (sample) to be yellow. In a bag with 20 candies, we would expect four candies to be yellow: one out of each five candies or the population proportion times the total number of candies in the sample = 0.20 * 20 = 4.0.  

#### Representative sample  
Because the share of yellow candies in the population represents the probability of drawing a yellow candy, we expect also 20% of the candies in our bag to be yellow. For the same reason we expect the shares of all other colors in our sample bag to be equal to their shares in the population. As a consequence, we expect a random sample to be in principle representative of the population from which it is drawn. A sample is _representative_ of a population if variables in the sample are distributed in the same way as in the population. Of course, we know that a random samples is likely to differ from the population due to chance, so the actual sample that we have drawn is usually not representative of the population. But we should expect it to be representative, so we say that it is _in principle representative_ of the population. We can use the theory of probability to account for the misrepresentation in the sample that we draw.

#### Unbiased estimator  
Note that the expected value of the proportion of yellow candies in the bag (sample statistic) equals the true proportion of yellow candies in the candy factory (population statistic). This is always, that is, by definition, true for all sample statistics that are _unbiased estimators_ of the population statistic. By the way, we usually refer to the population statistic as a _parameter_.  

Most but not all sample statistics are unbiased estimators of the population statistic. Think, for example, of the number of yellow candies in the sample. This is surely not an unbiased estimator of the number of yellow candies in the population. Because the population is so much larger than the sample, the population must contain many more yellow candies than the sample. If we estimate the number in the population (the parameter) from the number in the sample---for example, we estimate that there are four yellow candies in the population of all candies---we are going to vastly underestimate the number in the population. This estimate is downward biased: It is too low.

In contrast, the proportion in the sample is an unbiased estimator of the population proportion. That is why we do not use the number of yellow candies to generalize from our sample to the population but we use the proportion of yellow candies instead. You probably already did this intuitively.  

Sometimes, we have to adjust the way in which we calculate a sample statistic to get an unbiased estimator. For example, we must calculate the standard deviation and variance, which is the square of the standard deviation, in the sample in a special way to obtain an unbiased estimate of the population standard deviation and variance. The exact calculation need not bother us because our statistical software will take care of that.  

#### Expected value

__interactive content: what do you expect is the mean of the sampling distribution? ; add statistics to the (empirical) sampling distribution: mean__

In the preceding paragraphs, we have noticed that the expectation or expected value of the proportion of yellow candies in the sample is equal to the proportion of yellow candies in the population. If you carefully inspect the sampling distribution, you will see that the expected value also equals the mean of the sampling distribution. The number of yellow candies that we have too many in samples with more than the expected number are equal to the number of yellow candies that we lack in the samples with less than the expected number of yellow candies. This makes sense: Excess yellow candies in some bags must be compensated for by a shortage in other bags.  

Thus we arrive at the definition of the _expected value_ of a random variable: It is the average of the sampling distribution of a random variable. The sampling distribution is an example of a probability distribution, so, more generally, the expected value is the average of a probability distribution. The expected value is also the _expectation_ of a probability distribution.  

__exercise/interactive content: If we throw two unbiased dice and sum the number of eyes, what is the expected value? Enter your answer before you simulate the sampling distribution. ; point out the sampling space, the probability ditribution and its mean in the simulated sampling distribution__

### 1.3 A Continuous Random Variable: Overweight And Underweight.

Let us now look at another variable: the weight of candies in a bag. The weight of candies is perhaps more interesting to the average consumer because it is related to the number of calories that a candy contains.  

#### Continuous variable  
Weight is a _continuous variable_ because we can always think of a new weight between two other weights. For example, consider two candy weights: 2.8 and 2.81 gram. It is easy to see that ther can be a weight inbetween these two values, for example, 2.803 gram. Between 2.8 and 2.803 we can discern an intermediate value such as 2.802. In principle, we could endlessly continue in this way even if our scales may not allow us to differentiate any further, e.g., between 2.80195661 and 2.80195662 gram. It is the principle that counts. If we can always think of a new value in between two values, the variable is continuous.  

#### Continuous sample statistic  
We are not interested in the weight of a single candy but in the average weight of all candies in our sample bag. The average candy weight in our sampe bag is our key sample statistic and we want to use this sample statistic to say something about average candy weight in the population of all candies. Can we do that?

The sample mean is an unbiased estimator of the population mean, so the average weight of all candies in the population (at the factory) is the average of the (candy weights in the) sampling distribution. And this is the average weight that we expect in a sample drawn from this population (the expected value or expectation). So far, everything is the same as in the case of the proportion of yellow candies, which was a discrete random variable.  

#### Continuous sample statistic  
When we turn to the probabilities of getting samples with a particular average candy weight, we run into problems with a continuous sample statistic. If we would want to know the probability of drawing a sample bag with an average candy weight of 2.8 gram, we should exclude sample bags with an average candy weight of 2.81 gram, or 2.801 gram, or 2.8000000001 gram, and so on. In fact, we are very unlikely to draw a sample bag with an average candy weight of exactly 2.8 gram, that is, with an infinite number of zeros trailing 2.8. In other words, the probability of such a sample bag is for all practical purposes zero and negligible.  

This applies to every average candy weight, so all probabilities are virtually zero. As a consequence, we cannot construct a probability distribution of the sampling space, that is, of all possible outcomes. By the way, note that this is also impossible because we have an infinite number of possible outcomes. After all, we can always find a new weight between two selected weights.  

#### p-values  
We can solve this problem by looking at a range of values instead of a single value. We can sensibly talk about the probability of having a sample bag with an average candy weight of at least 2.8 gram or at most 2.8 gram. We choose a threshold, in this example 2.8 gram, and determine the probability of values above or below this threshold. We can also use two thresholds, for example the probability of an average candy weight between 2.75 and 2.85 gram. This is probably what you were thinking of when I referred to a bag with 2.8 gram as average candy weight.  

The probability of values up to (and including) the threshold value or the threshold value and higher are called _p-values_. The probability of values up to (and including) the threshold value is known as the _left-hand p-value_ and the probability of values above (and including) the threshold value is called the _right-hand p-value_. Why did I put _(and including)_ between parentheses? It does not really matter whether we add the exact boundary value (2.8 gram) to the probability on the left or on the right because the probability of getting a bag with average candy weight at exactly 2.8 gram (with a very long trail of zero decimals) is negligible.  

__interactive content (practice reading probabilities in a continuous sampling distribution (approximated by a normal distribution) representing average candy weight in a sample bag on the x-axis ; select thresholds with 2 slider(s) showing the average weight, probability/surface to left and to right and between): what is the probability of buying a bag with average candy weight of 2.8 gram or more/less/between ? ; is this a left-hand p-value, a right-hand p-value or neither? ; what is the minimum average weight of the 10% bags with the heaviest candies?__

#### Probabilities always sum to 1
While you were playing with the interactive content, you may have noticed that displayed probabilities always add up to one. This is true for every probability distribution as you have learned before. In addition, you may have realized that the probabilities can also be interpreted as proportions or percentages. The probability that a sample bag contains candies with average weight over 2.9 gram is equal to the proportion of samples in the sampling distribution with average candy weight over 2.9 gram. Thus, we can use probabilities to find the threshold values that separate the top ten percent or the bottom five percent in a ditribution.  

### 1.4 Concluding Remarks

In this chapter, we have used a very simple example to understand sampling distributions. It is more simplistic than the real-life applications that you are going to deal with but you can use it if you want to understand more complicated, real-life applications. Just translate the complicated application into the simple example (reason by analogy).  

For example, a communication scientist may want to know whether an anti-smoking campaign makes people more aware of the risks of smoking. For a random sample of people, exposure to the campaign and awareness of risks associated with smoking are measured. Using regression analysis, we can see that exposure is positively related to risk awareness in the sample. But may we conclude that it is also positively related with risk awareness in the population?  

__interactive content: a scattergram representing two variables in a population (exposure and awareness of risks) that are not correlated (or: randomly generated with either no or a small positive or negative slope) ; one small random sample from the population with a regression line and regression equation depicted ; question: 'What do you think is the slope of the regression line in the population?', answers: 'positive/increasing', 'negative/decreasing', 'horizontal/no relation' ; a button to create three more random samples with regression line and regression equation ; goal: to sensitize the student to the risks of fully trusting the sample result__

If we translate this to the simple candy bag example, we realize that the outcome in our sample need not be the true population value. After all, we could very well draw a bag with less or more than twenty percent yellow candies. The regression coefficient, then, can be positive in our sample even if there is no relation or a negative relation in the population. How we decide on this, is discussed in later chapters.  

#### Samples as observations
Perhaps the most confusing aspect of sampling distributions is the fact that samples are the observations. We are accustomed to think of observations as measurements on empirical _things_ such as people or candies. We perceive each person or each candy as a unit or entity and we observe a characteristic that may change across entities (a variable), for example the color of a candy or the weight of a candy. In contrast, we observe samples (observations) and measure a sample statistic as the (random) variable in a sampling distribution. Each sample adds one observation to the sampling distribution and its sample statistic value is the value added to the sampling distribution.  

#### Means at three levels
In our example, the sample statistic is a proportion, namely the proportion of candies that are yellow. The horizontal axis of the histogram of the sampling distribution represents the proportion of yellow candies. This is fully in line with our research question. If we want to know whether all colors are equally distributed, we are interested in sample proportions, not in properties of individual candies.  

Things become a little confusing if we are interested in the sample mean, for example, the average weight of candies in a sample bag. Now we have means at three levels: the population, the sampling distribution, and the sample. Point them out in the interactive content below.  

__interactive content: a normally distributed population with the mean indicated as a vertical line/value on the X-axis, a normally distributed sampling distribution (as a histogram?) of sample means (surface under the curve) with the mean of the sampling distribution as a vertical line/value on the X-axis, and a sample as a histogram with its mean as a vertical line/value on the X-axis ; Q: 'The sample mean is found in two places. Click on both places.' A1: the mean in the sample distribution and the corresponding bar in the histogram of the sampling distribution, add feedback for all choices__

The sampling distribution, here, is a distribution of sample means but the sampling distribution itself also has a mean, which is called the expected value or expectation of the sampling distribution. Don't get confused! The mean of the sampling distribution is the average of the average weight of candies across all possible samples. This mean of means has the same value as our third mean, namely the average weight of the candies in our sample.  

Think of the three means as a hamburger. The top and bottom part of a hamburger are both made of bread. They represent the same type of thing, namely candies and their weight. The middle part of the hamburger, however, is a completely different type of food. The meat holds the two halves of the bun together. In a similar way, the sampling distribution connects the population to the sample but it is of a very different nature. It consists of observations on samples, for example, bags of candies instead of single candies. Whereas the unit of measurement is grams in both the population and sample in this example, the unit of measurement in the sampling distribution is average sample weight.

__add picture of hamburger population/sampling distribution/sample?__

The sampling distribution sticks to the population because the population statistic (parameter), for example, the average weight of all candies, is equal to the mean of the sampling distribution. It sticks to the sample because it tells us which sample means we will find with what probabilities. The sampling distribution is the vital link connecting the sample to the populaiton. We need it to make statements about the population based on our sample.  

### 1.5 Take-Home Points  

* Statistics of random samples from the same population vary but some results are more probable than other results.  
* The sampling distribution of a sample statistic tells us the probability of drawing a sample with a particular value of this statistic or a particular minimum/maximum value.  
* If a sample statistic is an unbiased estimator of a parameter, the parameter value is the average of the sampling distribution, which is called the expected value or expectation.  
* For discrete sample statistics, the sampling distribution tells us the probability of individual sample outcomes. For continuous sample statistics, it tells us the p-value: the probability of drawing a sample with an outcome that is at least or at most a particular value.  

## Chapter 2. Probability Models: How Do I Get A Sampling Distribution?
> Key concepts: bootstrapping, sampling with replacement, exact approach, modelling, approximation with a theoretical probability distribution, binomial distribution, (standard) normal distribution, probability density function, (Student) _t_ distribution, _F_ distribution, chi-squared distribution, condition checks, in/dependent samples, sample size, homogeneity of variance, expected values, independent samples, dependent samples, paired samples, matched samples.

In the previous chapter, we have drawn very many samples from a population to obtain the sampling distribution of a sample statistic. The procedure is quite simple: Draw a sample, calculate the desired sample statistic, for example, the proportion of yellow candies or the average candy weight in the sample, add the sample statistic value to the sampling distribution, and repeat this thousands of times.  

Although this procedure is simple, it is not practical. In our research project, we would have to draw thousands of samples and administer a survey to each sample or collect data on the sample in some other way. This requires too much time and money to be of any practical value. So how do we create a sampling distribution if we only collect data for one sample? This chapter presents three ways of doing this: bootstrapping, exact approaches, and theoretical approximations.

### 2.1 The Bootstrap Approximation of the Sampling Distribution

The first way to obtain a sampling distribution is still based on the idea of drawing very many samples. However, we only draw one sample from the population for which we collect data. As a next step, we draw a large number of samples from our initial sample. The samples drawn in the second step are called _bootstrap samples_. For each bootstrap sample, we calculate the sample statistic of interest and we collect them into the sampling distribution. We usually want about 5,000 bootstrap samples for our sampling distribution.  

In the interactive content below, draw bootstrap samples to build a sampling distribution of the proportion of yellow candies in a sample. Afterwards, indicate whether you think the bootstrap sampling distribution resembles the true sampling distribution.
__interactive content: Q: 'Does the bootstrap sampling distribution resemble the true sampling distribution?' A: 'Yes', 'No' ; show a large (quite representative) sample ; button to draw one sample with replacement at a time, showing the proportion of yellow candies in the sample and adding it to the histogram of the sampling distribution, which already shows the true sampling distribution (binomial distribution) as a light histogram in the background ; after using this button five times, enable another button to add hundreds or thousands of samples to the hitogram in one go__

The _bootstrap_ concept refers to the story in which Baron von M?nchhausen saves himself by pulling himself and his horse by his hair out of a swamp. In a similar miraculous way, the bootstrap samples resemble the sampling distribution even though they are drawn from a sample instead of the population. This miracle requires some explanation and it need not work always as we will discuss in the remainder of this section.  

![Baron von M?nchhausen pulls himself and his horse out of a swamp.](figures/Munchhausen.png)

#### Sampling With And Without Replacement  

__interactive content: Q: what are the differences between sampling with and without replacement? ; show sampling source (original sample) with 10 observations ; buttons to create four (bootstrap) samples with or without replacement ; example: candy color (colored circles with an ID number)__

As we will see in a later chapter, the size of a sample is very important to the shape of the sampling distribution. The sampling distribution of samples with twenty observations can be very different from the sampling ditribution of samples of size forty. To construct a sampling distribution from bootstrap samples, the bootstrap samples must be exactly as large as the original sample.  

How can we draw many different bootstrap samples from the original sample if each bootstrap sample must contain the same number of observations as the original sample? If we allow every observation in the original sample to be sampled only once, each bootstrap sample contains all observations of the original sample, so it is an exact copy of the original sample. Thus, we cannot create different bootstrap samples.  

By the way, we are accustomed to this type of sampling, which is called _sampling without replacement_. If we sample a person, we do not put this person back in the population so she or he can be sampled again. We want our respondents to fill out our questionnaire only once or participate in our experiment only once.  

If we do allow the same person to be sampled more than once, we sample _with replacement_. The same person can occur more than once in a sample. Bootstrap samples that are sampled with replacement from the original sample can vary because they need not contain all observations in the original sample. Some observations may not be sampled while other observations are sampled several times. You may have noticed this in the previous interactive content. Sampling with replacement, we can obtain different bootstrap samples from the the original sample and still have bootstrap samples of the same size as the original sample.  

#### Calculating Probabilities With Replacement  
You may wonder whether it is OK to sample with replacement. The short answer: Yes it is. We usually calculate probabilities as if we sampled with replacement. Suppose we want to calculate the probability of a sample with ten candies, two of which are yellow. Further suppose that the probability of sampling a yellow candy is .2. The probability of drawing a sample with two yellow candies in a bag with ten candies is then calculated as .2 * .2 * .8 * .8 * .8 * .8 * .8 * .8 * .8 * .8: twice the probability of drawing a yellow candy and eight times the probability of drawing a candy that is not yellow.  

In this calculation, we assume that the probabilities remain the same while we are sampling. The probability of sampling a yellow candy is .2 whether we sample the first or tenth candy. This means that the proportion of yellow candies in the population remains the same, namely 20%. This assumption is very convenient because it simplifies the calculation of probabilities.

#### Calculating Probabilities Without Replacement  
However, the number of yellow candies is reduced by one after we have drawn the first yellow candy (unless it is immediately replaced by a new yellow candy in the factory). The probability of drawing a second yellow candy should then be less than 20%. If the population is large, the decrease in the probability is too small to be in any way relevant. For example, if we have a population of a million candies and 20% are yellow, the probability of drawing the first yellow candy is 200,000 / 1,000,000 = .2. The probability of drawing the second yellow candy would be 199,999 / 999,999 = 0.1999992 ; the difference between the two probabilities (0,0000008) is negligible. 

So is it OK to sample with replacement? Yes, it is, because we usually calculate probabilities as if we sampled with replacement. Then, is it not OK to sample without replacement as we normally do in research? It is OK to sample with replacement as long as the population is much larger than the sample. If the population is much larger, the probabilities more or less remain the same during the sampling process, so calculating probabilities as if the probabilities do not change is not a problem.

#### Restrictions To Bootstrapping  

In the interactive content, answer the question and then select several sample sizes to compare bootstrap sampling distributions to the true sampling distribution. Pay attention to the distribution of colors in the sample that you draw.  

__interactive content: Q: 'Does the bootstrap sampling distribution always reflect the true sampling distribution?' A: 'Yes, always.', 'Yes, if the sample is representative.', 'Yes, if the sample is not too small', 'Yes if the sample is not too large' ; allow the user to change sample size (initially very small) ; button to draw a sample of the specified size from a uniformly distributed population (five candy colors), show the sample and collect the proportion of yellow candies for 1,000 bootstrap samples into a hstogram , which already shows the true sampling distribution (binomial distribution) as a light histogram in the background__

We can create a sampling distribution by sampling from our original sample with replacement. It is hardly a miracle that we obtain different samples with different sample statistics if we sample with replacement. The real miracle is that this bootstrap distribution resembles the true sampling distribution that we would get if we draw lots of samples directly from the population.  

Does the miracle always happen? No, it need not happen. First, the original sample that we have drawn from the population must not be very small. We cannot draw many different samples from a small sample. In this situation, the bootstrap distribution cannot resemble the true sampling distribution.  

Second, the original sample must be more or less representative of the population. The variables of interest in the sample should be distributed more or less the same as in the population. If this is not the case, the sampling distribution may be biased, giving a distorted view of the true sampling distribution.  

A sample is more likely to be representative of the population if the sample is drawn in a truly random fashion and if the sample is larger. But we can never be sure. There always is a chance that we have drawn a sample that does not reflect the population well. This is the main problem with the bootstrap approach to sampling distributions.  

#### Any Sample Statistic Can Be Bootstrapped  
The big advantage of the bootstrap approach (_bootstrapping_), however, is that we can get a sampling distribution for any sample statistic that we are interested in. Every statistic that we can calculate for our original sample can also be calculated for each bootstrap sample. The sampling distribution is just the collection of the sample statistics calculated for all bootstrap samples.  

Bootstrapping is more or less the only way to get a sampling distribution for the sample median, for example, the median weight of candies in a sample bag. We may create sampling distributions for the wildest sample statistics, for example the difference between sample mean and sample median squared. I would not know why you would be interested in the squared difference of sample mean and median but there are very interesting statistics that we can only get at through bootstrapping. A case in point is the strength of an indirect effect in a mediation model.  

__interactive content: Q: 'What do you expect to be the mean of the sampling distribution of the difference between the mean and median weight of candies in a sample bag?' A: the value of the population mean, the value of the sample mean, the value of the population median, the value of the sample median, the value of the difference between the population mean and median, the value of the difference between the sample mean and median ; create and show (as a histogram) a skewed population distribution of candy weights with mean and median values as well as a skewed sample with slightly different mean and media ; button to draw one bootstrap sample with its mean, median, and difference of mean and median, this difference stored in a histogram that also shows the mean of all values in the histogram ; after five single bootstrap samples, automatically add the difference for 1,000 bootstrap samples to the histogram__

### 2.2 Exact Approaches to the Sampling Distribution

A second approach to constructing a sampling distribution has implicitly been demonstrated in the preceding section on bootstrapping. There, we calculated the true sampling distribution of the proportion of yellow candies in a sample from the probabilities of the colors. If we know or think we know the proportion of yellow candies in the population, we can exactly calculate the probability that a sample of ten candies includes one, two, three, or ten yellow candies. See the section on discrete random variables for details.  

The calculated probabilities of all possible sample statistic outcomes gives us an exact approach to the sampling distribution. Note that I use the word _approach_ instead of _approximation_ here because the obtained sampling distribution is no longer an approximation, that is, more or less similar to the true sampling distribution. No, it is the true sampling distribution itself.  

#### Exact Approaches For Categorical Data
An exact approach can be used to create a sampling distribution for one proportion such as the proportion of yellow candies in the example above. Exact approaches are also available for the association between two categorical (nominal or ordinal) variables in a cross-tab: Do some combinations of values on the two variables occur relatively frequently? For example, are yellow candies more often sticky than red candies? If so, we conclude that color and stickiness are associated. The _Fisher-exact test_ is an example of an exact approach to the sampling distribution of the association between two categorical variables.  

#### Computer-intensive
The exact approach can be applied to discrete variables because they have a limited number of values. Discrete variables are usually measured at the nominal or ordinal level. If the number of categories becomes large, a lot of computing time can be needed to calculate the probabilities of all possible sample statistic outcomes. Exact approaches are said to be _computer-intensive_. It is usually wise to set a limit to the time you allow your computer to work on an exact sampling distribution because otherwisethe problem may keep your computer occupied for hours or days.  

### 2.3 Theoretical Approximations Of The Sampling Distribution

Because bootstrapping and exact approaches to the sampling distribution require quite a lot of computing power, these methods were not practical in the not so very distant pre-computer age. In those days, mathematicians and statisticians discovered that many sampling distributions look a lot like known mathematical functions. For example, the sampling distribution of the sample mean can be quite similar to the well-known bell-shape of the _normal distribution_. The mathematical functions are called _theoretical probability distributions_.  

__interactive content: compare a theoretical probability distribution (normal distribution) to a simulated sampling distribution (of sample means from a symmetrical population with large variance) and show the threshold value for the upper 5% for both distributions (to draw attention to the tails as important areas): does fit depend on sample size? ; renew the simulated sampling distribution several times, is the simulated sampling distribution better or the theoretical probability distribution?__

The normal distribution is a mathematical function linking continuous scores, e.g., a sample statistic such as the average weight in the sample, to p-values, that is, to the probability of finding at least or at most this score. Such a function is called a _probability density function_. A probability density function does not have probabilities or any other quantity on the vertical (Y) axis. Instead, the surface between the graph's curve and the horizontal (X) axis represent probabilities. The total surface is assumed to be one because probabilities always sum to one.  

We like to use a theoretical probability distribution as an approximation of the sampling distribution because it is convenient. The computer can calculate probabilities from the mathematical function for us. We also like theoretical probability distributions because they usually offer a plausible argument about chance and probabilities.  

#### Reasons for a bell-shaped probability distribution
The bell shape of the normal distribution, for example, makes sense. We are just as likely to sample a bag with too heavy candies as a bag with too light candies, so the sampling distribution of the sample mean should be symmetrical. A normal distribution is symmetrical.  

In addition, we are more likely to sample a bag with an average weight that is near the true average candy weight in the population than a bag with candies that are much heavier or ligther than the true average. Bags with on average extremely heavy or extremely light candies may occur but they are extremely rare (we are very lucky or unlucky). From these intuitions we would expect a bell shape for the sampling distribution.  

From this argumentation, we conclude that the normal distribution is a reasonable model for the probability distribution of sample means. As a model, the theoretical probability distribution may actually give a better approximation of the sampling distribution than a sampling distribution created by drawing many samples from the population or from the initial sample as in bootstrapping. Sampling is always subjected to chance, so we may have accidentally drawn samples that do not cover the sampling distribution well.  

#### Restrictions to the use of theoretical probability distributions
Theoretical probability distributions, then, are plausible models for sampling distributions. They should have the same shape as the true sampling distributions at least under particular circumstances or conditions. If we use a theoretical probability distribution, we must assume that the conditions for its use are met. We have to check the conditions and decide whether they are close enough to the ideal conditions. _Close enough_ is of course a matter of judgement. In practice, rules of thumb have been developed to decide if the theoretical probability distribution can be used.  

The interactive content below shows an example in which the normal distribution is a good approximation for the sampling distribution of a proportion. How do you expect sample size and the value of the proportion in the population affect the shape of the sampling distribution? Do you expect that the normal distribution always fits the sampling distribution of the sample proportion? State you expectations and then check them in the interactive content by changing sampe size and population proportion.  

```{r normal_approx_sampling_proportion}
#interactive content: compare exact (binomial) sampling distribution to the normal distribution (with population proportion pi as mean and sqrt of pi(1 - pi)/N as standard deviaiton) for different sample sizes and population proportions ; additional question: why is the true sampling distribution (of the sample proportion) more symmetrical if the proportion is nearer to .5?
#perhaps adapt/simplify CLT_prop from Shiny-Ed on GitHub, https://github.com/ShinyEd/ShinyEd/tree/master/CLT_prop
```

Do theoretical probability distributions fit the true sampling distribution? As you may have noticed with the interactive content, this is not always the case. In general, theoretical probability distributions fit sampling distribution better if the sample is larger. In addition, the value of the parameter may be relevant to the fit of the theoretical probability distribution. The sampling distribution of a sample proportion is more symmetrical like the normal distribution if the proportion in the population is nearer .5.  

This illustrates that we often have several conditions for a theoretical probability distribution to fit the sampling distribution that we should evaluate together. In the example of proportions, a large sample is less important if the true proportion is closer to .5 but it is more important for true proportions that are more distant from .5. The rule of thumb for using the normal distribution as the sampling distribution of a sample proportion combines the two aspects by multiplying them and requiring the resulting product to be larger than five.  If the probability of sampling a yellow candy is .2 and our sample size is 30, the product is .2 * 30 = 6, which is larger than five. So we may use the norma distribution as approximation of the sampling distribution.  

Table @. Rules of thumb for the use of theoretical probablity distributions.

what am I analyzing?  | Distribution  | Minimum sample size | Other requirements
:-------------------| :---------- | :------------------: | :-----------------
One mean  | Normal distribution | > 100 | OR variable is normally distributed in the population
One mean  | _t_-distribution | > 30  | OR variable is normally distributed in the population
Means of two groups  | _t_-distribution | > 30  | OR variable is normally distributed in each group's population (different versions for equal and unequal population variances)
Means of one group at different times  | _t_-distribution | > 30  | OR variable is normally distributed in each group's population
Means of three or more groups  | _F_-distribution | all groups are more or less of equal size  | OR all groups have the same population variance
One proportion  | Normal distribution | >= 5 divided by @  | 
__et cetera (To Be Done)__ |   |   | 

Apart from the normal distribution, there are several other theoretical probability distributions. We have the _t_-distribution for one or two sample means, regression coefficients, and correlation coefficients, the _F-distribution_ for comparison of variances and comparing means for three or more groups (analysis of variance, ANOVA), and the _chi-squared distribution_ for frequency tables and cross-tabs.  

For most of these theoretical probability distributions, sample size is important. The larger the sample, the better. There are additional conditions that must be satisfied such as the distribution of the variable in the population. The rules of thumb are summarized in a table (above).  

#### Checking conditions  
Rules of thumb about sample size are easy to check once we have collected our sample. In contrast, rules of thumb that concern the scores in the population cannot be easily checked because we do not have information on the population. If we already knew the population, why would we draw a sample and and do the research in the first place?  

We can only use the data in our sample to make an educated guess about the distribution of the variable in the population. For example, if the scores in our sample and clearly not normally distributed, it is hard to believe that the scores in the population are. We could use a histogram of the scores in our sample with a normal distribution curve added to evaluate whether a normal distribution applies. Sometimes, we have statistical tests to draw inferences about the population from a sample that we can use to check the conditions. We discuss tests in a later chapter.  

#### More complicated sample statistics: differences  
Up to this point, we have focused on rather simple sample statistics such as the proportion of yellow candies or the average weight of candies in a sample. The table, however, contains more complicated sample statistics. If we compare two or more groups, for example, the average weight of yellow and red candies, the sample statistic for which we want to have a sampling distribution must take into account both the average weight of yellow candies and the average weight of red candies. The sample statistic would be the difference between the averages of the two samples. 

__interactive content{compare bootstrapping difference between mean and median}: demonstrate the construction of a sampling distribution for mean differences for independent samples; two populations (normal distributions): weight of red candies and yellow candies; comparing average weight: first draw a sample from each population, then calculate difference of the two means and store this difference in a sampling distribution__

If we draw a sample from both the yellow and the red candies in the population, we may calculate the means for both samples and the difference between the two means. For example, the average weight of yellow candies in the sample bag is 2.76 gram and the average for red candies is 2.82 gram. For this pair of samples, the statistic of interest is 2.76 - 2.82 = -0.06, that is, the difference in average weight. If we repeat this many, many times and collect all differences between means in a distribution, we obtain the sampling distribution that we need.  

The sampling distribution of the difference between two means is similar to a _t_-distribution, so we may use the latter to approximate the former. It is important to note that we do not create separate sampling distributions for the average weight of yellow candies and for the average weight of red candies and then look at the difference between the two sampling distributions. Instead, we create one sampling distribution for the statistic of interest, namely the difference between means. We cannot combine different sampling distributions into a new sampling distribution.  

#### Independent samples  
If we compare two means, there are two fundamentaly different situations that are sometimes difficult to distinguish. Comparing the average weight of yellow candies to the average weight of red candies, we are comparing two samples that are _statistically independent_, which means that we could have drawn the samples separately.  

In principle, we could distinguish between a population of yellow candies and a population of red candies and sample yellow candies from the first population and separately sample red candies from the other population. Whether we sampled the colors separately or not does not matter. The fact that we could have done so implies that the sample of red colored candies is not affected by the sample of yellow candies or the other way around. The samples are statistically independent.  

This is important to how probabilities are calculated. Just think of the simple example of flipping two coins. The probability of having heads twice in a row is .5 times .5 that is .25 if the coins or unbiased and the result of the second coin does not depend on the result of the first coin. The second flip is not affected by the first flip. Imagine that a magnetic field is activated if the first coin lands with heads up and that this magnetic field increases the odds that the second coin will also be heads. Now, the second toss is not independent of the first toss and the probability of having heads up twice is larger than .25.  

#### Dependent samples  

The example of a manipulated second toss is applicable to repeated measurements. If we are interested in how quickly the yellow color fades when yellow candies are exposed to sun light, we would draw a sample of yellow candies once and measure the colorfulness of each candy at least twice: at the start and after some time interval. We would like to compare the average colorfulness of the second set of measurements to the average in the first set of measurements.  

__interactive content: Q: 'What, do you think, is of interest if we have repeated measurements?', A: 'The difference between the mean of all first scores and the mean of all second scores.', 'The mean of the difference between each first score and each second score.' ; demonstrate the construction of a sampling distribution for mean differences for paired/dependent samples; two populations (normal distributions): colorfulness of yellow candies at two times (surface below curve shaded from dark yellow to pale yellow) with their means; comparing average colorfulness: draw a sample unit by unit by clicking on a bar in the histogram of the first population ; show ID number on click in both populations ; show the difference score and store it as value in the sample ; keep a running mean on the difference scores in the sample ; continue until a sample of 10 observations has been created ; then give option to animate drawing another sample ; then give option to add/animate many more samples__

In this example, we are comparing two means, just like the yellow versus red candy weight example, but now the samples for both measurements are the same. It is impossible to draw the sample for the second measurement independently from the sample for the first measurement if we want to compare repeated measurements. Here, the second sample is fixed once we have drawn the first sample. The samples are _statistically dependent_; they create _paired samples_.  

As a consequence, probabilities have to be calculated in a different way, so we need a special sampling distribution. In the interactive content above, you may have noticed a relatively simple solution for two repeated measurements. We just calculate the difference between the two measurements for each candy in the sample and use the mean of this new difference variable as the sample statistic that we are interested in. For this sample statistic, we construct a sampling distribution. The _t_-distribution offers a good approximation of this sampling distribution if the sample is not too small. Note that we calculate the differences at the level of the observations (candies) now whereas we calculated the differences at the level of the sample (candy bag) in the situation with two independent samples.  

#### Matched samples
Samples that are not exactly the same as in the case of repeated measurements but that are still related, are dependent samples. For example, we may have investigated media literacy among children five years ago and now we want to know whether average media literacy among children has increased. We had better not inquire after the media literacy of the children in our sample drawn five years ago. These children have aged and media literacy is likely to increase with age. The children in our new sample should resemble the children in our previous sample on background variables that we think are important (_matched samples_), including age. We do not want differences in background to distort the comparison between the two samples.  

Our two samples consist of different children but the second sample cannot be drawn without taking into account the first sample, so the two samples are statistically dependent. If we want to test the mean difference in media literacy, we should apply the technique for paired samples. We should have one child in the new sample for each child in the previous sample, with relevant characteristics matching. We should calculate the difference in media literacy for each pair of matching children to obtain a sampling distribution of differences that tells us which differences we should expect with what probabilities in our sample.  

The actual sampling distribution can become quite complicated but we need not worry about that. If we choose the right technique, our statistical software will take care of this. Of course, we should check whether the conditions are met for approximating the sampling distribution with a theoretical probability distribution. If the conditions are met, we can use the probabilities provided by our statistical software for the two techniques of statistical inference that we are going to discuss in the next two chapters: estimation and null hypothesis testing.  

### 2.4 Take-Home Points  

* We may simulate/create an exact or bootstrap sampling distribution in simple situations or if we have a lot of computing power.  
* For a bootstrap sampling distribution, we need about 5,000 bootstrap samples from our original sample.  
* We can often approximate the sampling distribution of a sample statistic with a known theoretical probability distribution.  
* Approximations work well only under conditions, which we have to check.
* Conditions usually involve the size of the sample, sample type (independent vs. dependent/paired), and the shape or variance of the population distribution.  
* Samples are independent if we could have drawn a sample for one group without taking into account the sample for another group of observations. Otherwise, the samples are dependent or paired.

## Chapter 3. Estimating a Parameter: Which Population Values Are Plausible?
> Key concepts: point estimate, interval estimate, confidence (level), precision, standard error, critical value, degrees of freedom, confidence interval, uncertainty.

In this chapter, we set out to make educated guesses of a population value (parameter) based on our sample. Our first guess will be a single value for the population value. We merely guess that the population value is equal to the value of the sample statistic. This guess is the best we can make but it is most likely to be wrong. Our second guess uses the sampling distribution to make a statement about the approximate population value. More precisely, we calculate an interval for which we are confident that it includes the population value. The wider the interval, the more confident we are that it contains the true population value but, at the same time, the less precise our guess. This type of guessing is called _estimation_.

### 3.1 Point Estimate

__interactive content: try to do better than the sample: estimate te population value (of the proportion of yellow candies) using some other value than the sample proportion ; repeat for 10 (to 20?) samples ; note that each sample is drawn from a different population with candy proportion within the range .10 .40 (uniformly distributed) ; result: sum of mistakes (difference between true proportion and estimated proportion) over all samples for the sample versus the student__

If we have to give one value for the population value, our best guess is the value of the sample statistic. For example, if 18% of the candies in our sample bag are yellow, our best guess of the proportion of yellow candies in the population of all condies from which this bag was filled, is .18. What other number can we give if we only have our sample? This type of guess is called a point estimate and it is used a lot.  

Of course, the sample statistic is the best estimate of the population value only if the sample statistic is an unbiased estimator of the population value. As we have learned in Chapter 1, the true population value is equal to the mean of the sampling distribution for an unbiased estimator. This mean is the expected value for the sample. In other words, an unbiased estimator neither systematically overestimates the population value nor does it systematically underestimate the population value.  With an unbiased estimator, then, there is no reason to prefer a value higher or lower than the sample value as our estimate of the population value.  

Even though the value of the statistic in the sample is our best guess, it is very unlikely that our sample statistic is exactly equal to the population value (parameter). The recurrent theme in our discussion of random samples is that a random sample differs from the population because of chance during the sampling process. The precise population value is highly unlikely to actually appear in our sample. The sample statistic value is our best point estimate but it is nearly certain that it is wrong. It may be slightly or strongly off the mark but it will hardly ever be spot on. For this reason, it is better to estimate a range within which the population value falls. Let us turn to this in the next section.  

### 3.2 Interval Estimate For The Sample Statistic  

The sampling distribution tells us the probabilities of finding particular sample statistic values in the case of a discrete random variable, for example, the number of yellow candies in a sample bag. It tells us the probability of finding a range of scores in the case of a continuous random variable such as the average weight of candies in a sample bag. The sampling distribution allows us to calculate the probability of a sample with a particular (minimum or maximum) value for the sample statistic. We can also use it to calculate the probability of finding a sample statistic between two values, that is, within a range of values. We use this range in our interval estimate.  

The average or expected value of a sampling distribution, we have learned before, is equal to the population value if the estimator is unbiased. For example, the mean weight of yellow candies averaged over very many samples is equal to the mean weight of yellow candies in the population. For an interval estimate, we now select the sample statistic values that are closest to the population value, hence, to the mean of the sampling distribution. Between which borders are situated the sample statistic values that are closest to the population value?  

To find the borders, we must choose the proportion of sample statistic values that we are interested in. Which part of all samples do we want to include? A popular proportion is 95%, so we want to know the border values that include 95% of all samples that are closest to the population value. Say, for example, that 95% of all possible samples in the middle of the sampling distribution have an average candy weight ranging from 2.7 to 2.9 gram.  

__interactive content: move the slider until you have found the interval containing 95% of all samples, which are closest to the (true) population value ; example: average weight approximated with a normal distribution ; right border can be moved with the slider, left border automatically moves in the opposite direction and the proportion of the surface between the borders is shown ; note that the slider cannot be moved left of the center of the distribution because it would cross the left-hand border__

The proportion .95 can be interpreted as a probability. Our sampling distribution tells us that we have 95% probability that the average weigth of yellow candies will lie between 2.7 and 2.9 gram in a random sample that we draw from this population. We now have border values, that is, a range of sample statistic values, and a probability of drawing a sample with a statistic falling within this range. The probability adds our (in)security into the estimation. We may be wrong to conclude that average candy weight in the population is between 2.7 and 2.9 gram but at least we know the probability that we are wrong.  

### 3.3 Precision, Standard Error, and Sample Size

The width of the estimated interval represents the _precision_ of our estimate. The wider the interval, the less precise our estimate. With a less precise interval estimate, we reckon with a wider variety of outcomes in our sample. If we want to predict something, however, we value precision. We rather conclude that the average weight of candies in the next sample we draw lies between 2.75 and 2.85 gram than betweem 1.5 and 4.1 gram. If we would be satisfied by a very imprecise estimate, we need not do any research at all. With a little knowledge about the candies that we are investigating, we could straightaway predict that the average candy weight is between zero and ten gram. The goal of our research is to make more precise estimations.  

__interactive content: Q: How can you decrease the width of the estimated interval? (Multiple answers can be correct), A: 'Select a lower probability', 'Select a higher probability', 'Draw a smaller sample', 'Draw a larger sample' ; effect of selected probability and of sample size (via standard error) on the sample (statistic) results that we expect to get ; depict the standard deviation of the generated sampling distribution__

There are several ways to increase the precision of our interval estimate, that is, obtain a narrower interval for our estimate. The easiest and least useful way is to decrease the probability that our estimate is correct. If we lower the probability that we are right, we can discard more possible sample statistic outcomes and focus on a narrower range of sample outcomes around the true population value. This method is not useful because we sacrifice our security or confidence that the range includes the outcome in the sample that we are going to draw. What is the use of a more precise estimate if we are less sure that it predicts correctly? Therefore, we usually do not change the probability and leave it at 95% or thereabouts (90%, 99%). We think it important to be quite sure that our prediction will be right.  

#### Sample size  
A less practical but very useful method of narrowing the interval estimate is increasing sample size. As you may have noticed while playing with the interactive content in the beginning of this section, a larger sample yields a narrower, that is, more precise interval. You may have expected intuitively that larger samples give more precise estimates because they offer more information. This intuition is correct.  

In a larger sample, an observation above the mean is more likely to be compensated by an observation below the mean. Just because there are more observations, it is less likely that we sample relatively high scores but no or considerably less scores that are relatively low. In other words, the larger the sample, the more the distribution of scores on a variable in the sample will resemble the distribution of scores on this variable in the population. It is more likely to be more representative of the population. As a consequence, a sample statistic value will be closer to the population value for this statistic.  

Larger samples are more similar to the population and therefore large samples drawn from the same population are more similar to one another. The result is that the sample statistic values in the sampling distribution are less varied and more similar. They are more concentrated around the true population value, which is the average of the sampling distribution. The sampling distribution is more peaked, so the middle 95% of all sample statistic values are closer to the center.  

#### Standard error  
The concentration of sample statistic values, such as average candy weight in a sample bag, is expressed by the standard deviation of the sampling distribution. Hitherto, we have only paid attention to the center of the sampling distribution, its mean, because it is the expected value in a sample and it is equal to the population value (if the estimator is unbiased). Now we start looking also at the standard deviation of the sampling distribution because it tells us how precise our interval estimate is going to be. The sampling distribution's standard deviation is so important that it has received a special name: the _standard error_.  

__interactive content: How wrong are point estimates? ; draw random samples from a population, calculate the (absolute) difference between sample mean and population mean, and depict a (root) cumulative average (squared) difference for all preceding samples ; show the standard error as a line in this graph__

The word _error_ reminds us that the standard error tells us the size of the error that we are likely to make (on average under many repetitions) if we use the value of the sample statistic as a point estimate for the population value. Let us assume, for example, that the standard error of the average weight of candies in a sample bag is 0.2. Loosely stated, this means that the average difference between the true average candy weight and the average candy weight in a sample is 0.2 if we draw very many samples from the same population. By the way, the standard deviation does not give us the ordinary average difference but it gives us the square root of the average of squared differences. But this detail is irrelevant to how we interpret the standard error.  

The smaller the standard error, the more sample statistic values resemble the true population value, the more precise our interval estimate with a given probability, say 95%. Because we like more precise interval estimates, we prefer small standard errors over high standard errors. In theory, it is easy to obtain smaller standard errors: just increase sample size. In practice, however, it is both time-consuming and expensive to draw a very large sample. Usually, we want to settle on the optimal size of the sample, namely a sample that is large enough to have interval estimates at the precision that we need but as small as possible to save on time and expenses. We return to this matter in Chapter 5.  

### 3.4 Critical Values  

__interactive content: as above (??) but with double labeling of the x-axis: raw scores and standard errors? ; see that critical value remains the same (even if raw scores change) when the sample size is changed (and, hence, the standard error)?__

In the interactive content (above), we approximate the sampling distribution with a theoretical probability distribution, namely the normal distribution. If we use a theoretical probability ditribution, the selected probability level corresponds to a particular outcome value in the theoretical probability distribution.  

#### Standardization and _z_-scores  
The most simple situation arises if we _standardize_ the sampling distribution: subtract the average of the sampling distribution from each sample mean in this distribution and divide the result by the standard error. Thus, we transform the sampling distirbution into a standardized distribution of _z_-scores. The mean of the new standardized _Z_-variable is allways zero.  

If we use the normal distribution for standardized scores, that is, the standard-normal distribution or _z_-distribution, there is a single _z_-value that marks the boundary between the top 2.5% of all samples and the bottom 97.5%. This _z_-value is 1.96. If we combine this value with -1.96, separating the bottom 2.5% of all samples from the rest, we obtain an interval [-1.96, 1.96] containing 95% of all samples that are closest to the mean of the sampling distribution.   

In a standard-normal or _z_-distribution, 1.96 is called a _critical value_. It separates the 95% sample statistic outcomes that are closest to the parameter, hence that are most likely to appear, from the 5% that are furthest away and least likely to appear. There are also critical _z_-values for other probabilities, for example, 1.64 for the middle 90% of all samples and 2.58 for the middle 99%. 

#### Interval estimates from critical values and standard errors  
Critical values in a theoretical probability distribution tell us the boundaries or range of the interval estimate expressed in standard errors. In a normal distribution, 95% of all sample means are situated no more that 1.96 standard errors from the population mean. If the standard error is 0.2 and the population mean is 2.8 gram, we have 95% probability that the mean candy weight in a sample that we draw from this population lies between 2.408 gram (this is 2.8 minus 1.96 times 0.2) and 3.192 gram. You may notice that this is not a very narrow and precise interval estimate because the upper border is more than thirty percent higher ([1 3.192/2.408]*100%) than the lower border.  

Critical values make it easy to calculate an interval estimate if we know the standard error. Just take the population value and add the critical value times the standard error to obtain the upper limit of the interval estimate. Subtract the critical value times the standard error from the population value to obtain the lower limit. In the case of a normal distribution, it is extremely easy because there is a fixed critical value for each probability, for example 1.96 for 95% probability, which one could learn by heart.  

#### Degrees of freedom (_df_)  
For other theoretical probability distributions, the situation is slightly more complicated because we have several different critical values for the same probability level. Smaller samples usually require higher critical values, just like categorical variables with more values. More generally, the critical values depend on the _degrees of freedom_ in the sample. We need not be concerned with the exact meaning of degrees of freedom or how the degrees of freedom are calculated for a sample if we want to aply a particular theoretical probability distribution. Our statistical software takes care of this for us.  

### 3.5 Confidence Interval for a Parameter

Working through the preceding sections, it may have occurred to you that it is all very nice to be able to estimate the value of a statistic in a new sample with a particular precision and probability but that we are not really interested in doing that. Instead, we want to estimate the value of the statistic in the population. For example, we don't care much about the average weight of candies in our sample bag or in the next sample bag that we may buy. We want to make a statement about the average weight of candies in the population. How can we do that?  

In addition, you may have realized that we know the precise population value, for example, average candy weight, if we know the sampling distribution. The average of the sampling distribution, after all, is equal to the population mean for an unbiased estimator. In the preceding paragraphs, we acted as if we knew the sampling distribution. If we know the sampling distribution, we know the population value, so why would we care about estimating an interval?  

So our problem is this. We want to estimate a population value using probabilities. For probabilities we need the sampling distribution but for the sampling distribution, we must know the population. In the exact approach to the sampling distribution of the proportion of yellow candies in a sample bag, for example, we must know the proportion of yellow candies in the population. If we know the population proportion, we can exactly calculate the probability of getting a sample bag with a particular proportion of yellow candies. But we don't know the population proportion of yellow candies; we want to estimate it.  

The interactive content below shows a lot of standard normal ditributions that we could use to approximate the sampling distribution of the average candy weight in a sample. Of course, we can only use one. How can you pick the one that we need?  
__interactive content: Q: 'What do we need to know to use the standard normal distribution as approximation of the sampling distribution of a sample mean? (Multiple answers can be correct)' A: 'Sample size', 'Sample mean', 'Sample standard deviation', 'Standard error', 'Population mean', 'Population standard deviation' ; show an X axis labeled 'sample mean' and with a scale from 0 to 5 gram ; initially, add 20 normal curves with (5) different (population) means and (4) different standard errors ; show inputs for sample size, sample mean, and so on, where the user can inut a numeric value within restricted ranges (the number can be changed afterwards but it cannot be removed) ; if the student enters a number for the sample mean, show the sample mean in the figure (but don't change the normal curves because the sample mean is irrelevant) ; if a population mean is entered, replace the mean of all curves by this number and redraw (so all curves are at the same center; one could reduce the number of curves to the number of different standard errors present) ; similarly, a number entered for standard error or for sample size AND sample standard deviation (calculate standard error as SD divided by square root of sample size), replaces the range of standard errors for the curves and redraw (optionally: retain only one curve for each population mean if none has been fixed yet ; updating a previous numerical input by a new one should change the position or shape of the curve)__
![Example layout](figures/CI_2.png)

In a similar way, a theoretical probability distribution can only be used as an approximation of a sampling distribution if we know some characteristics of the population. For example, we know that the sampling distribution of sample means has the bell shape of a normal or _t_-distribution but we need the population mean to know where the center of the sampling distribution is located. Again, we need to know the population mean to use a theoretical probability distribution to estimate the population mean. This sounds like a problem that only Baron von M?nchhausen can solve. How can we drag ourselves by the hair out of this swamp?  

By the way, we also need the standard error to know how peaked or flat the bell shape is. The standard error can usually be estimated from the data in our sample. If you have carefully experimented with the options in the interactive content above, you should have some ideas. But let us not worry about how the stadard error is being estimated. We just want to estimate the population mean.  

#### Imaginary population values  
How do we solve the M?nchhausian problem that we must know the population mean to estimate the population mean. More generally, that we must know the parameter that we want to estimate? The solution is that we just select a lot of imaginary population values and calculate the interval within which the sample mean is expected to fall with a chosen probability, usually 95%. Next, we check if the mean of the sample that we have actually drawn falls within this interval. If it does, we conclude that this (imaginary) population mean is not at odds with the sample that we have drawn. In contrast, if our sample mean falls outside the interval, we conclude that this population mean is unlikely according to our sample. In this way, we can find the population means that are consistent with our sample, that is, for which we are sufficiently likely to draw a sample with a sample mean like the one we got.  

Click repeatedly on the upper line (population) to find the highest and lowest value of the population mean for which the sample mean is in the interval of sample means that have 95% probability to occur. Try to minimize the number of clicks you need.  
__interactive content: Q: none ; draw two horizontal lines, the top line labeled 'population' and the bottom line labeled 'sample', both lines with a numerical scale (0-5) ; generate a sample mean and standard error within a particular range, say 1-4 for the sample mean and the standard error in a range convenient to have interval estimates within the 0-5 or -1-6 range ; mark the sample mean on the lower line and show the value of the standard error somewhere in the app ; if the user clicks on the upper line, the corresponding number is shown as an (imaginary) population mean ; the 95% probability interval for the sample mean is shown as a horizontal line segment on top/near the lower line ; if the line segment overlaps with the sample mean, the selected population value is marked by a green dot and the line segment is green, they are red dot otherwise ; also show the z-value of the sample mean for the chosen population mean ; a 'New Try' button generates new values to restart the interaction (repeat the assignment with different values for the sample mean and standard error to emphasize that the z-values remain the same)__
![Example layout](figures/CI_1.png)

While playing with the interactive content (above), you may have noticed the _z_-values of the sample mean for the lowest and highest population means for which the sample mean is still within the interval. When you added the lower bound of the population means, the sample mean  had a _z_-value of 1.96 while it had a _z_-value of -1.96 for the highest population mean in the range. It is not a coincidence that we find thecritical values of the standard-normal distribution here. We are using the standard-normal distribution to approximate the sampling distribution of the sample mean. The critical _z_-value 1.96 marks the upper border of the interval containing 95% of all samples with means closest to the population mean and -1.96 marks the lower border. A distance of 1.96 standard errors, then, is the maximum distance between a population mean and a sample mean that belongs to the 95% sample means closest to the population mean.  

#### Interval of population values from standard error and critical value  
Instead of manually looking for the boundaries of the interval within which the population means fall that are likely given the current sample mean, we can calculate them using the critical values. If we take the value of the sample statistic that we obtained, for example, the mean weight of candies in our sample bag, we just add the product of the critical value and the standard error to this mean to obtain the upper limit of the interval. We subtract this product from the sample mean to obtain the lower limit of the interval.  

Of course, we must use an appropriate theoretical probability distribution (see Table @ in Chapter 2) and the critical values associated with this distribution. In addition, we must know the standard error if we work with a normal (_z_) or _t_-distribution but not if we work with an _F_-distribution or chi-squared distribution. If our theoretical probability distribution does not work with a standard error, we must use the bootstrap approach to find the lower and upper limits of the interval of parameter values for which the value of the statistic in the current samples is sufficiently likely. There are different ways of calculating these limits but we will not discuss them here.  

#### Confidence interval
The upper and lower bounds for the population mean or, more generally, the parameter that we want to estimate, yield an interval for the parameter. We use this as the interval estimate of the parameter. This interval is linked to a probability, for example, 95%, but we should realize very well that this is NOT the probability that the parameter has a particular value or that it falls within the interval. The parameter is not a random variable because it is not affected by the random sample that we draw. For example, how could the sample that we draw change the average weight of all candies? The parameter has one value, which is either within or outside the interval that we have constructed. We just don't know. But we do know that our sample is more likely for population values within the interval.  

We use the term _confidence_ instead of probability when we use this interval to estimate a parameter. We say that we are 95% confident that the parameter falls within the interval. The interval is called a _confidence interval_ and we usually add the confidence level, for example, the 95% confidence interval (abbreviated: 95%CI) of the average weight of candies in the population ranges from 2.4 to 3.2 gram. An average candy weight between 2.4 and 3.2 gram is plausible given the sample that we have drawn. In our reports, we say that:  
> We are 95% confident that the average candy weight in the population is between 2.4 and 3.2 gram.  

__interactive content: How often does a 95%-confidence interval contain the population value (parameter)? Can you explain this? ; generate 100 samples and mark the ones that do not contain the parameter__

The more precise meaning of a confidence interval is rather complicated. If we draw very many samples from the same population, 95% of the sample means would differ less from the population mean than the critical value times the standard error. This is true because of the way critical values are defined. In other words, if we construct the 95% confidence interval for each sample mean, 95% of all samples will have confidence intervals that contain the true population mean. Unfortunately, we have no clue whether or not our single sample belongs to the 95 percent of 'lucky' samples with confidence intervals containing the true population value. We can only hope and be confident that this is the case.  

### 3.6 Take-Home Points  

* If a sample statistic is an unbiased estimator, we can use it as a point estimate for the value of the statistic in the population.  
* A point estimate may come close to the population value but it is most certainly not correct.  
* A 95% confidence interval is an interval estimate of the population value: We are 95% confident that the population value lies within this interval. Note that confidence is not a probability!  
* A larger sample or a lower confidence level yields a narrower, that is, a more precise confidence interval. The sample statistic is more likely to be closer to the population value. We have less uncertainty in our results.  

### Further Reading
<table><tr><td>Jerzy Neyman introduced the concept of a confidence interval in 1937: Neyman, J. (1937). Outline of a Theory of Statistical Estimation Based on the Classical Theory of Probability. _Philosophical Transactions of the Royal Society A_. 236: 333-380. [doi:10.1098/rsta.1937.0005](https://dx.doi.org/10.1098%2Frsta.1937.0005)</td><td>  
![Jerzy Neyman](figures/jerzyneyman.jpg)</td></tr></table>

## Chapter 4. Testing A Null Hypothesis: Am I Right Or Am I Wrong?
> key concepts: research hypothesis, statistical null and alternative hypothesis, nill hypothesis, test statistic, p-value, significance level (Type I error rate), Type I error, inflated Type I error, capitalization on chance, one-sided and two-sided tests (and tests to which this distinction does not apply), rejection region.  

In the preceding chapter, we have learned that a confidence interval contains the population values that are plausible given the sample that we have drawn. In the current chapter, we narrow this down to the question whether the expectation of the researcher about the population is plausible. The expectation is usually called a (research) hypothesis and it must be translated into statistical hypotheses about a population value (parameter): a null hypothesis and an alternative hypothesis. We test the null hypothesis in the following way. We construct a sampling distribution in one of the ways we have learned in Chapter 2 using the value specified in the null hypothesis as the imaginary population value. In other words, we act as if the null hypothesis is true. Then we calculate the probability of drawing a sample such as the one we have drawn or a sample that differs even more from a population for which the null hypothesis is true. If this p-value is very low, say, below 5%, we reject the null hypothesis because our sample would be too unlikely if the null hypothesis is true. In this case, the test is statistically significant. The probability threshold that we use is called the significance level of the test.

### 4.1 A Binary Decision

The overall goal of statistical inference is to increase our knowledge about a population if we only have a random sample from the population. In the preceding chapter, we estimated population values that are plausible considering the sample that we have drawn. For example, we looked for all plausible average weights of candies in the population using information about the weight of candies in our sample bag. This is what we do when we estimate a population value.  

Estimation is one of the two types of statistical inference, the other is null hypothesis testing. When we estimate a population value, we do not use our previous knowledge about the world of candies or whatever other subject we are investigating. We can be completely ignorant about the phenomenon that we are investigating. This approach is not entirely in line with a conception of scientific progress going by the name of _empirical cycle_, in which scientists develop theories about the empirical world, test these theories against data collected from this world, and improve their theories if they are contradicted by the data.  

Hypothesis testing is more in line with this conception of scientific progress. It requires the researcher to formulate an expectation about the population, usually called a _hypothesis_. If the hypothesis is based on theory and previous research, the scientist uses previous knowledge. As a next step, the researcher tests the hypothesis against data collected for this purpose. If the data contradict the hypothesis, the hypothesis is rejected and the researcher has to improve theory. If the data does not contradict the hypothesis, it is not rejected and, for the time being, the hypothesis is accepted and we need not change our theory.  

Hypothesis testing, then, amounts to a choice from two options: accept or reject the hypothesis. This is a binary decision between believing that the population is as it is described in the hypothesis or believing that it is not. This is quite a different outcome than the range of plausible population values that we obtain when we estimate a confidence interval. Nevertheless, hypothesis testing and confidence intervals are tightly related as we will see later on in this chapter.

### 4.2 Formulating Statistical Hypotheses

A _research hypothesis_ is a statement about the empirical world that can be tested against data. Communication scientists, for example, may hypothesize that:  
* a television station reaches half of all households in a country,  
* media literacy is below a particular standard (5.5 on a 10-point scale) among children,  
* opinions about emigration are not equally polarized among young and old voters,  
* the celebrity endorsing a fundraising campaign makes a difference to people's willingness to donate,  
* more exposure to brand advertisements increases brand awareness,  
* and so on.  

As these examples illustrate, research hypotheses seldom refer to statistics such as means, proportions, variances, or correlations. Still, we need statistics to test a hypothesis. The researcher must translate the hypothesis into a new hypothesis specifying a statistic in the population, for example, the population mean. The new hypothesis is called a _statistical hypothesis_.  

Translating the research hypothesis into a statistical hypothesis is perhaps the most creative part of statistical analysis. This is just a fancy way of saying that it is difficult to give general guidelines stating which statistic fits which research hypothesis. All we can do is give some hints.  

#### Proportions: shares  
This book covers tests for four types of statistics: proportions, means, variance/standard deviations, and associations. A proportion is the statistic best suited to test research hypotheses addressing the share of a category or entity in the population. The hypothesis that a television station reaches half of all households in a country provides an example. All households in the country constitute the population. The share of the television station is the proportion or percentage of all households watching this television station.  

If we want to use a statistic, we need to know the variable and observations for which the statistic must be calculated. In this example, a household does or does not watch the television statement, so our variable is a dichotomy with the two categories ("No, does not watch this station", "Yes, watches this station") usually coded as 0 versus 1.  

Each household provides an observation, namely either the score 0 or the score 1 on this variable or no score if there are missing values. To test the research hypothesis that a television station reaches half of all households in a country, we have to formulate a statistical hypothesis about the proportion---of households viewing this television statement---in the population---of all households in this country. For example, the researcher's statistical hypothesis could be that the proportion in the population is 0.5.  

#### Means: levels
Research hypotheses that focus on the level of scores are usually best tested with the mean or another measure of central tendency. For example, the hypothesis that media literacy is below a particular standard (5.5 on a 10-point scale) among children refers to a level: the level of media literacy scores. The hypothesis probably does not argue that all children have a media literacy score below 5.5. Instead, it means to say that the overall level is below this standard. The center of the distribution offers a good indication of the general score level.  
For a numeric (interval or ratio measurement level) variable such as the 10-point scale in the example, the mean is a good measure of the distribution's center. In this example, our statistical hypothesis would be that average media literacy score of all children in the population is below 5.5.  

#### Variance: (dis)agreement
Although rare, research hypotheses may focus on the variation in scores rather than on the score level. The hypothesis about polarization provides an example. Polarization means that we have scores well above the center and well below the center rather than all scores concentrated in the middle. If voters' opinions about emigration are strongly polarized, we have a lot of voters strongly in favour of admitting emigrants as well as many voters strongly opposed to admitting emigrants.  

For a numeric variable, the variance or standard deviation---the latter is just the square root of the former---is the appropriate statistic to test a hypothesis about polarization. The research hypothesis concerns the variation of scores in two groups: young versus old voters. The statistical hypothesis would be that the variance in opinions in the population of young voters is different from the variance in the population of old voters.  

#### Association: relations between characteristics  
Finally, research hypotheses may address the relation between two or more variables. Relations between variables are at stake if the research hypothesis states or implies that one (type of) characteristic is related to another (type of) characteristic. For example, if the identity of the endorser makes a difference to the willingness to donate, the endorser to whom a person is exposed (one characteristic) is related this person's willigness to donate. Another example: If exposure to brand advertisements increases brand awareness, a person's brand awareness is positively related to this person's exposure to advertisements for the brand.  

The statistical name for a relation between variables is _association_, so we may use a measure of association as the key statistic in our statistical hypotheses. Association comes in two related flavours: a difference in score level between groups or the predominance of particular combinations of scores on different variables.  

__interactive content: spot combinations of scores on two variables in a scatterplot (continuous and discrete variables) that are relatively rare (to focus on the 'empties') in a particular type of association ; refreshing the notions of positive and negative associations as well as associations that are neither ; include average differences between groups to illustrate that here too, particular combinations are rare r relatively frequent?__

#### Score level differences  
The relation between the endorser's identity and willingness to donate is a specimen of the first flavour. All people are confronted with one of the celebrities as endorser of the fund-raising campaign. This is captured by a categorical variable: the endorsing celebrity. The categorical variable clusters people into groups: One group is confronted with Celebrity A, another group with Celebrity B, and so on. If the celebrity matters to the willingness to donate, the general level of donation willingness should be higher in the group exposed to one celebrity than in the group exposed to another celebrity.  

Thus, we return to statistics needed to test research hypotheses about score levels, namely measures of central tendency. If willingness to donate is a numeric variable, we can use group means to test the association between endorsing celebrity (grouping variable) and willingness to donate (score variable). The statistical hypothesis would then be that group means are not equal in the population of all people. If you closely inspect Table @@, you will see that we prefer to use a _t_-distribution if we compare two groups whereas we use analysis of variance if we have three or more groups. In this example, test choice depends on the number of versions of the fundraising campaign that we investigate: two or more?  

#### Combinations of scores
The other flavour of association is the situation that particular combinations of scores on different variables are much more common than other combinations of scores. Think of the hypothesis that brand awareness is related to exposure to advertisements for the brand. If the hypothesis is true, people with high exposure and high brand awareness should occur much more often than people with high exposure and low brand awareness or low exposure and high brand awareness. The two variables here are exposure and brand awareness. One combination of scores on the two variables is high exposure combined with high brand awareness. This combination should be more common than high exposure combined with low brand awareness.  

Measures of association are statistics that put a number to the pattern of group score levels or combinations of scores. The exact statistic that we use depends on the measurement level of the variables. For numerical variables, measured at the interval or ratio level, we use Pearson's correlation coefficient or the regression coefficient. For ordinal variables with quite a lot of different scores, we use Spearman's rank correlation. For categorical variables, measured at the nominal or ordinal level, chi-squared indicates whether variables are statistically associated. The larger chi-squared, the less probable it is that the variables are not associated in the population. If variables are not associated, they are said to be _statistically independent_.  

__interactive content: Table @@. Overview of tests with conditions/requirements and SPSS commands ; interactive decision/flow chart - show complete diagram and pressing a button or clicking a cell hides all choices/cells that are not up- and downstream from this cell ; Q: For each of the examples, find the appropriate statistical test by clicking the correct cells in the columns from left to rigth.__

### 4.3 The Null and Alternative Hypothesis

In the preceding section, I have referred to statistical hypotheses without further qualification. There are, however, at least two different statistical hypotheses: the null hypothesis (_H_~0~) and the alternative hypothesis (_H_~1~ or _H_~A~). For reasons that will be explained in the next section, the _null hypothesis_ must equate the test statistic to a single value. For example, the statistical hypothesis that the proportion of all households in the population reached by a television station is .5 equates the population proportion to .5. In contrast, the statistical hypothesis that the average media literacy score of all children in the population is below 5.5 does not equate the population mean to one particular value. It is hypothesized to be smaller than 5.5 but it can be any number below 5.5.  

If the research hypothesis is not the null hypothesis, it is the _alternative hypothesis_. The alternative hypothesis covers all situations not covered by the null hypothesis and the other way around. The null hypothesis that the proportion of all households in the population reached by the television station is .5 is linked to the alternative hypothesis that the proportion is not .5. Thus, we cover all possible outcomes.  

The research hypothesis that average media literacy score of all children in the population is below 5.5, is an example of an alternative hypothesis. Most of the research hypothesis examples in the previous section are alternative hypotheses because they do not equate the statistic with a particular value:  
* If opinions about emigration are hypothesized to be different for young and old voters, the variances are hypothesized to be unequal in the population. But the research hypothesis does not state how unequal.  
* If the celebrity endorsing a fundraising campaign makes a difference to the willingness of people to donate, the average willingness is not hypothesized to be equal for all groups but, again, the size of the difference is not specified.  
* If we hypothesize that more exposure to brand advertisements increases brand awareness, we expect the correlation or regression coefficient to be positive, that is, larger than zero. But we have not hypothesized a particular value, so the research hypothesis represents the alternative hypothesis.  

#### Research hypotheses tend to be alternative hypotheses  
It is not a coincidence that most of the research hypothesis examples are alternative hypotheses instead of null hypotheses. This is quite usual in social research even though it is not necessarily so as some statistics textbooks would make us believe. Often, our theories tell us to expect differences or changes but not the size of differences or changes.  

If the research hypothesis is the alternative hypothesis, we have to formulate the null hypothesis ourselves. This is very important, because it is this hypothesis that is actually tested as we will see in a later section. Some examples:  
* If the alternative hypothesis states that variances or group means are unequal in the population, the null hypothesis would be that they are equal in the population.  
* An alternative hypothesis expecting a correlation between exposure and brand awareness requires the null hypothesis that there is no such association in the population.  

#### Nill hypothesis
Null hypotheses are quite often stating that there is no difference or no association in the population. But how do these null hypotheses equate the parameter of interest to a single number as null hypotheses should? They equate the statistic to zero. This type of null hypothesis is called a _nill hypothesis_ or just plainly the nill. 

A null hypothesis on association in the population simply assigns the value zero to the measure of association in the population. For example, Spearman's rho or Pearson's correlation between exposure and brand awareness are hypothesized to be zero in the population. Zero for a measure of association always means that there is no association.  

This also applies to the regression coefficient (_b_ in the regression equation): If it is zero, the predictor variable is useless for predicting the outcome variable. In these cases, the null hypothesis really specifies a null. If statistical software does not report the null hypothesis that is being tested, you may assume that it equates the parameter of interest to zero.  

Null hypotheses stating that means or variances are equal can be regarded as hypotheses that the difference between the means is zero or, equivalently, that the quotient of the two variances---if we divide one variance by the other---is one. We can assign a value to the difference or quotient, so we are actually testing differences or quotients.  

Hypothesis tests, then, require sampling distributions for differences, associations, and quotients. In Chapter 2, we have seen that there are theoretical probability distributions for differences and associations, for example, _t_-distributions, and quotients (_F_-distributions). And if we do not have a theoretical probability distribution, we can use bootstraping to obtain a sampling distribution.  

### 4.4 One-Sided And Two-Sided Tests

__interactive content: illustrating two-sided versus one-sided tests__

The research hypothesis that average media literacy is below 5.5 in the population represents the alternative hypothesis because it does not fix the hypothesized population value to one number. The accompanying null hypothesis must state that the population mean is 5.5 or higher. This null hypothesis is slightly different from the ones we have encountered so far, which equated the population value to a single value, usually zero. Our current null and alternative hypotheses are called _one-sided_ or one-tailed because the null hypothesis can only be rejected if the sample statistic is at one side of the spectrum: only below (left-tailed) or only above (right-tailed) a particular value. A test of a one-sided null hypothesis is called a _one-sided test_. 

In a one-sided test of the media literacy hypothesis, the researcher rules out the possibility that average media literacy among children is 5.5 or higher. The researcher brings prior knowledge about the world to bear that has convinced her that media literacy among children can only be lower than 5.5 on average. If there is a possibility that the score may also be higher than 5.5, the research hypothesis should have been that media literacy is not 5.5 among children. Then, the statistical hypotheses would have been _two-sided_ because both a sample statistic well below 5.5 and a sample statistic well above 5.5 would have rejected the null hypothesis (a _two-sided test_). 

#### Boundary value as hypothesized population value  
You may wonder how a one-sided null hypothesis equates the parameter of interest with one value as it should. The special value here is 5.5. If we can reject the null hypothesis that the population mean is 5.5 because our sample mean is sufficiently lower than 5.5, we can also reject any hypothesis about a population mean higher than 5.5 because the difference between that population mean and our sample mean is even larger than in the test against a population mean of 5.5. So the boundary value of a one-sided null hypothesis is the value we use for a one-sided test.  

#### One-sided -- two-sided distinction is not always relevant  
Note that the difference between one-sided and two-sided tests is only useful if we test a statistic against a particular value or if we test the difference between two groups. In the first situation, we may rule out the possibility that the population value is higher (or lower) than the hypothesized value if we have good reasons to believe that is can only be lower (or higher). In the second situation, we may expect that one group can only score higher than the other group and not the other way around.  

In contrast, we cannot meaningfully formulate a one-sided nul hypothesis if we are comparing three groups or more. Even if we expect that Group A can only score higher than Group B and Group C, what about the difference between Group B and Group C? If we can't have meaningful one-sided null hypotheses, we cannot meaningfully distinguish between one-sided and two-sided null hypotheses.  

#### Formulate statistical hypotheses in advance
Statistical hypotheses are important because they specify the statistic that we use in our test and they determine whether we do a two-sided or one-sided test. We have to formulate the statistical hypotheses before we have a look at our sample. After all, we want to give the sample data a fair chance to prove our hypothesis wrong. A statistical test is useless if we already know the result in the sample and adjust our hypotheses to that information.  

Finally, you should note that scientific papers usually report the research hypothesis but not the statistical hypotheses. The statistical hypotheses of a test are supposed to be know to all researchers. Make sure you know them as well!

### 4.5 p-Value and Significance Level ($\alpha$)

The purpose of formulating a null hypothesis is that we can use the value specified in the null hypothesis as a hypothetical population value. This saves us the trouble of looking for plausible population values, which we have to do if we estimate a confidence interval. Testing a null hypothesis, we just act as if the null hypothesis is true. This allows us to create one sampling distribution that will tell us how plausible our sample is if the null hypothesis would be true. If average media literacy of children in the population really is 5.5, how plausible is it to draw a sample with 3.9 as average media literacy?  

Note that we can only construct a sampling distribution if we have a single value for the parameter. If we would think that average media literacy is 5.4 in the population, we have to construct a different sampling distribution than if the hypothesized average is 5.5. For this reason, the null hypothesis must equate the parameter to one value as we have assumed in the preceding sections.  

__interactive content: p-values, Type I Error, and significance level in a sampling distribution (one-sided test) under a null hypothesis (vary null hypothesis?)__

#### p-value
If our sample statistic is a continuous variable, for example, average candy weight, we know that it does not make sense to calculate the probability of finding exactly the sample mean that we obtained. In Chapter 1, we have learned that we work with p-values in this situation, for example, the probability of finding this sample mean or a mean that is even more distant from the hypothesized population mean.

The question now is: How large must the difference be between the value that we expect according to our null hypothesis (the hypothesized population value) and the value that we observe in our sample before we stop believing that the null hypothesis is true? If our null hypothesis expects no correlation between advertisement exposure and brand awareness, how large should the correlation be in our sample before we reject the null hypothesis?  

The answer is that the difference should be so large that it is very improbable that we would draw a sample with the observed correlation from a population without any correlation. In statistical terms, the p-value must be very low if the null hypothesis is true. With a very low p-value, our sample is too unexpected to sustain our belief that the null hypothesis is true.

#### Statistical significance  
How low must we go? A commonly accepted threshold value is .05 (5%). If the p-value is .05 or less, we decide that our sample is too improbable and implausible to be drawn from a population for which the null hypothesis is true. Therefore, we reject the null hypothesis and we say that the test is _statistically significant_. If p is low, the null must go and the test is statistically significant. This is the golden rule of null hypothesis testing (although some argue that the gold of this rule is fake and poisonous, see Chapter 5/6?).  

#### Conditional probability  
It is important to remember: The p-value that we calculate is a probability __under the assumption that the null hypothesis is true__. It is a _conditional probability_. Compare it to the probability that we throw sixes with a dice. This probability is one out of six under the assumption that the dice is unbiased. Probabilities always rest on assumptions. If the assumptions are violated, we cannot calculate probabilities. If the dice is biased, we don't know the probability of throwing sixes. In the same way, we have no clue whatsoever of the probability of drawing a sample like the one we have if the null hypothesis is not true in the population. That's why it is so imported to specify a null hypothesis.  

#### Significance level And Type I error  
The threshold value, conventionally .05, is called the _significance level_ of the test. If the null hypothesis is true, for example, advertisement exposure and brand awareness are __not__ correlated in the population, but we happen to draw a sample with a correlation well above zero, our p-value is smaller than the significance level and we reject the null hypothesis.  

We must reject the null hypothesis in this situation; that is the rule of the game. But our conclusion that the null hypothesis is wrong, is an error in this situation. We don't know and don't believe we make this error but still we do. This error is called a _Type I error_: rejecting a hypothesis that is true.  

We cannot altogether avoid this error because samples can be very different from the population from which they are drawn. We have learned that in Chapter 1. The good thing is that we know the probability that we make this error. This probability is the significance level.  

But you should understand the exact meaning of probabilities. A significance level of .05 allows five percent of all possible samples to be so different from the population that we reject the null hypothesis if it is true. If we would draw a lot of samples and decide on the null hypothesis for each sample, we would reject a true null hypothesis in five percent of our decisions. So we have five percent chance of making a Type I error. We fix that probability when we select the confidence level of the test and we think that .05 is an acceptable probability for making this type of error. But we do not know wether our sample belongs to the five percent.  

#### p-values in one-sided tests  

__interactive content: p-values and sampling distributions for one-sided and two-sided tests in a normal (or t-) sampling distribution__

In the example of the correlation between exposure and awareness, we have only taken into account the right tail of the sampling distribution to calculate the p-value. If we expect a positive correlation between exposure to brand advertisements and brand awareness, we only reckon with the chance that the sample correlation is positive, well above zero, so we do a one-sided test.  

In this case, it makes sense to care only for the right tail of the sampling distribution. The entire probability of rejecting the null hypothesis while it is actually true, that is, the significance level, is located in the rigth tail of the sampling distribution. Of course, a one-sided test may alse focus solely on the left tail of the sampling distribution, for example, if we hypothesize that average media literacy of children is below 5.5.  

#### Significance level in two-sided tests  
What is the situation in a two-sided test, for example, if we test the null hypothesis that the variance in opinions about emigration among young people is different from the variation in opinions among old people? Here, we reckon with both possibilities: larger variance among young peoples and larger variance among old people. So there are two ways in which we may reject a null hypothesis of no difference in variance that is true (Type I Error): if the variation is sufficiently larger among young people or if it is sufficiently larger among old people. Both ways have an associated probability and their sum is the significance level.  

What we actually do is that we divide the significance level by two and use half of the probability for the left tail and the other half for the right tail of the sampling distribution. You will probably have noticed this when you were playing with the interactive content. If our significance level is .05, as it usually is, we use .025 as the maximum probability of finding a sample with a statistic that is so low that we would reject a null hypothesis that is true. We use the other .025 probability for drawing a sample with a statistic that is so high that we reject the null hypothesis.  

#### p-values in two-sided tests  
Just like the significance level, the p-value in a two-sided test has to take into account that we may reject the null hypothesis in two different ways.  

A p-value gives us the probability of drawing a sample with the value for the sample statistic that we have found in our current sample or a more extreme value. In other words, it is the probability of drawing a sample with a statistic that is at least as distant from the hypothesized population value as our current sample result.  

In a two-sided test, the distance can go in two directions: larger than the hypothesized value or smaller. As a consequence, the p-value must also cover samples at the opposite side of the sampling distribution. Because we assume equal p-values at both sides, we can double the one-sided p-value to obtain the two-sided p-value. And we can halve a two-wailed p-value to obtain the one-sided p-value.  

You do not need to worry about this if your statistics software reports the type of p-value that you need: one-sided or two-sided. If there is no choice between one- and two-sided tests, the reported p-value is always correct.  

However, if your software reports a one-sided p-value but you need a two-sided p-value, you have to double the reported p-value. In contrast, if the software reports a two-sided p-value while you need a one-sided p-value, you have to divide the reported p-value by two yourself because you do not want to take the probability in the other tail into account. Statistical software usually reports two-sided p-values, so this situation may occur.  

#### Unfounded one-sided null hypotheses  

__interactive content: p-values and sampling distributions for one-sided and two-sided tests and test results ; similar content as above but now with the decision to reject/accept the null hypothesis__

But be careful. If your one-sided test hypothesizes a positive association but your sample association is strongly negative, your one-sided test can never be significant even if the two-sided p-value is significant. The one-sided null hypothesis includes the outcomes that you thought were impossible: a negative correlation in this example. So if a negative correlation appears, you can never reject the null hypothesis, in other words, the test can never be significant.  

Don't use the reported two-sided p-value in this situation. Changing your null hypothesis during the analysis, even from one-sided to two-sided, is cheating. You will have to live with the fact that you excluded outcomes from your one-sided test that should not have been excluded.  

### 4.6 Test Statistic And Critical Value

Statistical software usually reports p-values, which are sometimes referred to as _significance_ or _Sig._. The decision about the null hypothesis is simple if you have the p-value. If the reported p-value is lower than the significance level, we reject the null hypothesis. Otherwise, we do not reject the null hypothesis.  

Sometimes, however, the software does not report a p-value but it reports only the value of the test statistic. A _test statistic_ is the random variable of a theoretical probability distribution. The test statistic is named after the theoretical probability distribution to which it belongs: _z_ for the standard-normal or _z_-distribution, _t_ for the _t_ distribution, _F_ for the _F_-distribution and, you already guessed, chi-squared for the chi-squared distribution.  

A test statistic is calculated from the sample statistic that we want to test, for example, the sample proportion, mean, variance, or association but it uses the null hypothesis as well. A test statistic more or less standardizes the difference between the sample statistic and the population value that we expect under the null hypothesis. The exact formula and calculation of a test statistic is not important to us. Just note that the larger the difference between observation (sample outcome) and expectation (hypothesized population value), the more extreme the value of the test statistic, the less likely to draw a sample with the observed outcome or an outcome even more different from the hypothesized value, the more likely we are to reject the null hypothesis.  

#### Calculation of a test statistic  
In Chapter 3, we encountered critical values of a theoretical probability distribution. A critical value separates all samples with a sample statistic outcome closest to the parameter from all samples with outcomes farthest away. We have seen that 1.96 is the critical value in a standard-normal or _z_-distribution that separates the 97.5% of all samples with the lowest sample statistics (e.g., sample means) from the 2.5% samples with highest sample statistics (e.g., sample means). We may as well say that 1.96 distinguishes between the 97.5% of all samples with the lowest _z_-values from the 2.5% samples with the highest _z_-values.  

The _z_-value, we now know, is the test statistic if we use the standard-normal distribution to approximate the sampling distribution. A critical value separates the test statistic values for the samples with results most similar to the hypothesis from those that are most dissimilar. So we can use critical values to test a null hypothesis. If the test statistic calculated for our sample is larger than the critical value, the probability of drawing such a sample from a population for which the null hypothesis is true is smaller than the significance level associated with the test statistic. In other words, the p-value is smaller than the significance level, so we must reject the null hypothesis.  

#### Null hypothesis test using the test statistic  
If we know the critical value associated with a significance level, we can simply compare the value of the test statistic reported for our sample with the critical value. If the reported value is larger than the critical value, for example, _z_ is larger than 1.96, or it is smaller than the negative critical value, for example, _z_ is smaller than -1.96, we conclude that the test is significant and the null hypothesis must be rejected. Values outside the critical values are called the _rejection region_.  

Of course, we must know the critical values for the theoretical probability distribution at the desired significance level if we want to test a null hypothesis using the value of the test statistic. It does not make sense to learn critical values by heart but there is one critical value that you may want to remember. The critical _z_-value of a two-sided test at 5% significance level is 1.96. That's almost two. The critical _t_-value of a two-sided test at 5% significance level also is around two if the sample is not too small (less than thirty.) So _z_ and _t_-values above two or below minus two signal statistical significance at the .05 level.  

This applies, for example, to standardized cell residuals in a cross-tabulation. If we hypothesize that two variables are statistically independent in the population, the standardized difference between the observed and expected cell value in a cross-tabulation has a sampling distribution that is approximately standard-normal. As a consequence, we must reject the null hypothesis of statistical independence for cells with a a _z_-value above 1.96 or below -1.96. For these cells, we conclude that they appear more (_z_ above 1.96) or less (_z_ below -1.96) in the population than warranted by statistical independence. These are supposedly the combinations of values in the population that represent the association between the two variables. They are the best candidates to describe the nature of the association.  

__interactive content: Q: What is the content of the relation between endorsing celebrity and decision to donate in the cross-tabulation? Mark the cells for which you reject the null hypothesis of statistical independence. ; generate a 4x2 crosstab (4 celebrity names by Yes-No donate) with cell frequncies under non-independence ; show observed and expected frequencies as well as standardized cell residuals ; on clicking a cell in the table, highlight it if the standardized residual is significant, otherwise notify mistake to the user__

__?? discuss interpretation of test on crosstabulation? ; celebrity endorsing a fundraising campaign makes a difference to people's willingness to donate__

### 4.7 Test Recipe
Testing a null hypothesis consists of several steps, which are now summarized as a recipe.

1. Specify the statistical hypotheses.  

In the first step, translate the research hypothesis into a null and alternative hypothesis. This requires chosing the right statistics for testing the research hypothesis and chosing between a one-sided or two-sided test if applicable.  

2. Select the significance level of the test.  

Before we execute the test, we have to choose the maximum probability of rejecting the null hypothesis if it is actuall true. This, as we know now, is the significance level. We select .05 (5%) as the significance level almost always. If we have a very large sample, e.g., some thousands of cases, we may select a lower significance level, for example, 0.01. See Chapter 5 for more details.  

3. Select how the sampling distribution is going to be created.  

Are you going to use bootstrapping, an exact approach, or a theoretical probability distribution? Theoretical probability distributions are the most common choice and we focus on them. Then, we have to know which theoretical probability distribution can be used for which test. If you are working with statistical software, selecting the right test, for example, a test on the means of two independent samples, you automatically select the right probability distribution, in this example, the _t_-distribution.  

4. Execute the test.  

Let your statistical software calculate the p-value of the test and/or the value of the test statistic. It is important that this step is executed after the first three steps. The first three steps should be done without knowledge of the results in the sample.  

5. Decide on the null hypothesis.  

Reject the null hypothesis if the p_value is lower than the significance level or the value of the test statistic is in the rejection region: larger than the positive critical value or lower than the negative critical value.  

6. Report the test results.  

The ultimate goal of the test is to increase our knowledge. To this end, we have to communicate our results both to fellow-scientists and to lay people who are interested in the subject of our research.  

Fellow scientists must be able to see the exact statistical test results. According to APA6, we should report the test statistic, the associated degrees of freedom (if any), the value of the test statistic, the p-value of the test statistic, and the confidence interval (if any). The APA6 requires a particular format for presenting statistical results and it demands that the results are included at the end of a sentence.  

The statistical results for a _t_-test on one mean, for example, would be:  

<center>_t_ (67) = 2.73, p = .004, 95%CI[4.13, 4.87]</center>  

* The degrees of freedom are put between parentheses directly after the name of the test statistic.  
* The value of the test statistic is 2.73 in this example.  
* The p-value is .004. Note that we report all results with two decimal places except probabilities, which are reported with three decimals. We are usually interested in small probabilities---less than .05---so we need the third decimal here.  
* The 95% Confidence Interval is 4.13 to 4.87, so with 95% confidence we state that the population mean is between 4.13 and 4.87.  

Not all tests have all items reported in the example above. For example, a _z_-test does not have degrees of freedom and _F_ or chi-squared tests do not have confidence intervals. Exact tests or bootstrap tests do not have a test statistic. Just report the items that your statistical software produces. But write them up in the right format.  

For fellow scientists and especially for lay people, it is important to read an interpretation of the results that clarifies both the subject of the test and the test results. Make sure that you tell your reader who or what the test is about:  
* What is the population that you investigate?  
* What are the variables?  
* What are the relevant sample statistics?  
* Which comparison do you make?  
* What are the test results see above)?  
* Are the results statistically significant and, if so, what are the estimates for the population?  
* If the results are significant, how larger are the differences or associations?  

A test on one proportion, for example, the proportion of all households reached by a television station, could be reported as follows:  

"The television station reaches significantly and substantially (_p_ = .61) more than half of all households in Greece, _z_ = 4.01, p < .001."  

The interpretation of this test on a proportion tells us the population ("all households in Greece"), the variable ("reaching a household") and sample statistic of interest (_p_ for proportion). It tells us that the result is statistically significant, which a fellow scientist can check with the reported p-value. Note that the actual p-value is well below .001, so if we would round it to three decimals, it would become .000. The latter notation suggests that the probability is zero but there is always some probability of rejecting thenull hypothesis if it is true. For this reason, APA6 wants us to report p < .001 instead of p = .000. Finally, the interpretation tells us that the difference from .5 is substantial. Sometimes, we can express the difference in a number, which is called the _effect size_, and give a more precise interpretation. See Chapter 5 for more information.  

If you have the value of the test statistic but not the p-value, you may report the significance level of the test instead of the p-value. In this case, you either report "p < .05" if the test is significant or "n.s." if the test is _n_ot _s_ignificant.  

### 4.8 Relation Between Null Hypothesis Test and Confidence Interval

In the previous section, we found that critical values are important both to null hypothesis testing and estmating confidence intervals. As a consequence, you will probably not be surprised to hear that the two types of statistical inference are related. The long and the short of it is that a 95% confidence interval contains all null hypotheses that would not be rejected with the current sample at the 5% significance level, two-sided.  

__interactive content: check the result of a null hypothesis test for parameter values inside and outside the 95% Confidence Interval ; show the sampling distribution? ; adapt interactive content created for Chapter 3 on confidence intervals__

Remember that the probability involved in a 95% confidence interval is not the probability that the population value is within the interval. The population value is a fixed number, not a random variable with a probability distribution. At least not in the approach to statistical inference that we follow here. For this reason, we say that we are 95% confident that the parameter lies within the 95% confidence interval instead of 95% probability that it is included in the interval.  

The probability refers to the sample that we have drawn. If the population value would have a value within the interval, our sample result is among the 95% samples that are most plausible to be drawn. But this is just another way of saying that a null hypothesis with this value for the parameter is sufficiently plausible (at a 5% sgnificance level) considering the sample that we have drawn. So we do __not__ reject the null hypothesis if it specifies any of the parameter values included in the 95% confidence interval.  

#### Testing a null hypothesis with a confidence interval  
If a 95% confidence interval contains all null hypotheses that would not be rejected with the current sample at the .05 significance level in a two-sided test, it is easy to execute a null hypothesis test if you know the confidence interval. If the populatin value specified in the null hypothesis is within the confidence interval, do not reject the null hypothesis. Otherwise, reject the null hypothesis.  

Let us imagine, for example, that the 95% Confidence Interval for average media literacy among children is 4.11 to 4.87. Our initial null hypothesis states that the average is at least 5.5. An average of 5.5 is clearly outside this 95% Confidence Interval, so this sample rejects the null hypothesis at the 5% significance level. We have already encountered this result in a previous section. Note, however, that our original test was one-sided but a confidence interval corresponds always to a two-sided test because it allows the parameter to be both smaller and larger than the hypothesized value.  

It is also clear that any null hypothesis specifying a population mean above 5.5 would be rejected. We could already infer that from our null hypothesis test. What we could not see there, however, is that a hypothesis specifying a population mean of 4.9 or 4.0 would also have been rejected in a two-sided test at the 5% level. A confidence interval gives much more information than a single null hypothesis test because it shows us test results for a range of null hypotheses.  

#### Testing a null hypothesis with bootstrapping  
Using the confidence interval is the easiest and sometimes the only way of testing a null hypothesis if we create the sampling distribution with bootstrapping. For example, we may use the median as the preferred measure of central tendency rather than the mean if the distribution of scores is quite skewed and the sample is not very large. In this situation, a theoretical probability distribution for the sample median is not known, so we resort to bootstrapping.  

The interactive content below shows a sampling distribution of sample medians for media literacy of teenagers. Determine a confidence interval and test the null hypothesis that teenager media literacy in the populaiton is 6.0.  
__interactive content: Q: Should we reject the null hypothesis that the population median of @ is @? ; combine bootstrapping app and CI app__

Bootstrapping creates an empirical sampling distribution: a lot of samples with a median calculated for each sample. We can use the interval containing the 95% of all sample medians in the middle of the distribution as our 95% Confidence Interval. If our null hypothesis about the population median is included in the interval, we accept the null hypothesis. Otherwise, we reject it.  

### 4.9 Capitalization On Chance

The relation between null hypothesis testing and confidence intervals may have given the impression that one could test a range of null hypotheses using just one sample and one confidence interval. For example, we could simultaneously test the null hypotheses that average media literacy among children is 5.5, 4.5, or 3.5. Just check if these values are inside or outside the confidence interval and your done?  

This impression is wrong. The probabilities that we calculate using one sample assume that we only apply one test on the data. If we test the original null hypothesis that average media literacy is 5.5, we run a risk of five percent to reject the null hypothesis if the null hypothesis is true.  

If we apply a second test to the same sample, for example, testing the null hypothesis that average media literacy is 4.5, we again run this risk of five percent. But the risk of rejecting a true null hypothesis at least once increases dramatically if we do two tests. This risk is 9.75 percent, namely one minus the risk of rejecting no true null hypothesis, which is 1 - .95 * .95 = .0975.  

The situation becomes even worse if we do three or more tests on the same sample. The total level of rejecting at least one null hypothesis that is true, increases well above the level that we want to have, namely five percent.  

The phenomenon that we are dealing with probabilities of making Type I errors that are higher (_inflated Type I errors_) than the significance level that we want to use, is called _capitalization on chance_. Applying more than one test to the same data is one way to capitalize on chance. If you do a lot of tests on the same data, it is very rare not to find some statisticaly significant results even if all null hypotheses are true.  

#### Capitalization on chance in post-hoc tests  
This type of capitalization on chance may occur, for example, in analysis of variance. To test the research hypothesis that the celebrity endorsing a fundraising campaign makes a difference to people's willingness to donate, we may organize an experiment using four versions of a video clip as the treatment, each clip featuring a different celebrity endorsing the campaign. This results in four groups of subjects, each group having an average score on their willigness to donate. As a first step, we test the null hypothesis that all groups have equal population means using an _F_-test.  

If this test is statistically significant, we reject the null hypothesis and conclude that at least two groups have different population means. The next question is: Which groups, that is, endorsement by which celebrities, display a different willingness to donate? To answer this question, we must do post-hoc _t_-tests on pairs of groups. With four groups, we have six pairs of groups, so we have six _t_-tests on independent means. The probability of rejecting at least one true null hypothesis of no difference is much higher than five percent if we use a significance level of five percent for each single _t_-test.  

#### Correcting for capitalization on chance  
We can correct in several ways for this type of capitalization on chance, for example, the Bonferroni correction. It merely divides the significance level that we use for each test by the number of tests that we do. In our example, we do six _t_-tests, so we divide the significane level of five percent by six. The resulting significance level for each _t_-test is .008. If a_t_-test's p-value is below .008, we reject the null hypothesis and we do not reject it otherwise.  

The Bonferroni correction is a rather coarse correction, which is not entirely correct. But it has a simple logic that directly links to the problem of capitalization on chance. Therefore, it is a good correction to understand the problem, which is the main goal we want to attain here. We skip better but more complicated alternatives to Bonferroni correction.  

Note that we need not apply a correction if we specified a hypothesis beforehand about the two groups that we expect to differ. In the example of celebrity endorsement, we would not have to apply the Bonferroni correction to the _t_-test on the mean difference between subjects confronted to Celebrity A and Celebrity C if we had hypothesized that the willingness to donate differs here. Of course, we could have skipped the analysis of variance and gone straight to the _t_-test with such a hypothesis.

#### Specifying hypotheses afterwards  
Capitalization on chance occurs if we apply different tests to the same variables in the same sample. This occurs in exploratory research in which we do not specify hypotheses beforehand but try out different predictors or different outcome variables. It occurs more strongly if we first have a look at our sample data and then formulate an hypothesis. Knowing the sample outcome, it is easy to specify a null hypothesis that will be rejected. This is plain cheating and it should be avoided at all times.

#### Advantages of using confidence intervals  
If we cannot use the confidence interval to test a range of null hypotheses, what is the added value of a confidence interval over a single null hypothesis test? The confidence interval tells us the plausible values of the parameter rather than whether or not it is one particular value. In this sense, we just know more with a confidence interval than with a null hypothesis tes.  

We can use the additional information if we would do a new null hypothesis test on a new sample. We would have a better idea of plausible null hypotheses. We could, for example, test whether media literacy is 4.49, the middle of the confidence interval. A confidence interval helps us to be more specific in the next research adressing our topic, whether this research is done by ourselves or by our colleagues.  

### 4.10 Take-Home Points  

* We use a statistical test if we want to decide on a null hypothesis: reject or not reject? Usually, is or isn't there an effect in the population?  
* The decision rules should be specified beforehand: the directionality of the test (one-sided or two-sided) and the significance level.  
* The null and alternative hypotheses always concern a population statistic. Together they cover all possible outcomes for the statistic. The null hypothesis always specifies one (border) value for the population statistic.  
* We reject the null hypothesis if a test is statistically significant. Which means that the probability of drawing a sample with the current or a more extreme outcome (more inconsistent with the null hypothesis) for the test statistic is below the significance level.  
* The 95% confidence interval includes all null hypotheses that would __not__ be rejected in a two-sided test at 5% significance level. It contains the population values that are not sufficiently contradicted by the data.  
* The calculated p-value is only correct if the same data is used for no more than one null hypothesis test and the null hypothesis was formulated beforehand.   
* If the same data is used for more null hypotheses tests, the probability of a Type I error increases. We obtain too many significant results (capitalization on chance).  

## Chapter 5: Which Sample Size Do I Need? Power!
> Key concepts: effect size, practical significance, Type II error, power, research hypothesis.  

At the start of a quantitative research project, we are confronted with a seemingly simple practical question: How large should our sample be? In some cases, the statistical test that we plan to use gives us rules of thumb for the minimum size that we need for applying this test. This may tell us the minimum sample size but not necessarily the optimal sample size. Even if we can apply the statistical test technically, sample size need not be sufficient for the test to signal the population differences or associations, for short, the effect sizes, that we are interested in.  

If we want to know the minimum sample size that we need to signal important effects in our data, things become rather complicated. We have to decide on the size of effects that we deem interesting. We also have to decide on the minimum probability that the statistical test will actually be significant if the true effect size in the population is of this size. This probability is the power of a test: the power to reject a null hypothesis of no effect if the effect in the population is of a size interesting to us. If we do not reject a false null hypothesis, we make a Type II error.  

Thinking about sample size thus confronts us with a problem that we have hitherto neglected, namely the problem of not rejecting a false null hypothesis. This problem is very important if the null hypothesis instead of the alternative hypothesis represents our research hypothesis. If the null hypothesis represents our alternative hypothesis, our expectations are confirmed if we do not succeed in rejecting the null hypothesis. But if we do not reject the null hypothesis, we cannot make a Type I error, namely rejecting a false null hypothesis, so the significance level of our test, the maximum probability of making a Type I error, is meaningless. We must know the probability of not rejecting a false null hypothesis---the power of the test---to express our confidence that our research hypothesis is true.  

### 5.1 Sample Size And Test Requirements  

Chapter 2 contains a table specifying the conditions that must be satisfied if we want to use a theoretical probability distribution to approximate a sampling distribution. Only if the conditions are met, the theoretical probability distribution resembles the sampling distribution sufficiently to use the theoretical probability distribution for estimation and hypothesis testing.  

Conditions often include sample size. If you plan to do a _t_-test, either on its own or in post-hoc tests after analysis of variance, each group should contain more than thirty obervations. So if you plan on doing _t_-tests, recruit more than thirty subjects for your experiment or more than thirty respondents for your survey and you are fine. Well, if you have to reckon with non-response, that is, subjects or respondents unwilling to participate in your research, you should address as many subjects or respondents necessary to have more than thirty observations in the end.  

Chi-squared tests require a minimum of five expected frequencies per category in a frequency distribution or cell in a crosstabulation. Your sample size should be at least the number of categories or cells times five to come even near this requirement. Regression analysis requires at least 15 observations per predictor variable in the regression model.  

The variation of sample size across groups is important to the analysis of variance. If the number of observations is more or less the same across all groups, we need not worry about the variances of the outcome variable in the population for the groups. To be on the safe side, then, it is recommended to design your sampling strategy in such a way that you end up with more or less equal group sizes if you plan to test analysis of variance (ANOVA) models.  

### 5.2 Effect Size  

We have learned that larger samples have smaller standard errors. Smaller standard errors yield (absolutely) larger test statistic values and larger test statistics have smaller p-values. In other words, a test on a larger sample is more often statistically significant. A larger sample offers more precision, so we will more often be convinced that the difference between our sample outcome and the hypothesized value is sufficient to reject the null hypothesis. For example, we would reject the null hypothesis that average candy weight is 2.8 gram in the population if the average weight in our sample bag is 2.75 gram and our sample bag is very big. But we may not reject the null hypothesis if we have the same outcome in a small sample bag.  

<img src="figures/metaldetector.png" alt="security metal detector." align="right"> Of course, the size of the difference between our sample outcome and the hypothesized value matters as well. If average candy weigth in our sample bag deviates more from the average weight that we expect according to the null hypothesis, we are more likely to reject the latter. The probability to reject a null hypothesis, then, depends both on sample size and the difference between what we expect null hypothesis) and what we find (sample outcome). With a larger sample, a smaller difference between outcome and expectation rejects the null hypothesis. If we think of our statistical test as a security metal detector, our test will pick up smaller amounts of metal if the sample is larger.  

Deciding on our sample size, we should ask ourselves this question: What effect size should produce a significant test result? In the security metal detector example, at what minimum quantity of metal should the alert sound? To answer this question, we should consider the practical aims and context of our research.  

#### Practical significance  
Investigating the effects of a new medicine on a person's health, we may require some minimum level of health improvement to make the new medicine worthwhile medically or economically. If a particular improvement is clinically important, it is _practically significant_. Then, we want our test to be statistically significant if the average true health improvement in the population is at least of this size. We would not want to accept the null hypothesis of no improvement in this situation.  

For media interventions such as health, political, or advertisement campaigns, one could think of a minimum change of attitude affected by the campaign given campaign costs. A choice between different campaigns could be based on their efficiency in terms of attitudinal change per unit of costs.  

Note the important difference between practical significance and statistical significance. Practical significance is what we are interested in. If the new medicine is sufficiently effective, we want our statistical test to signal it. In the security metal detector example: If a person carries too much metal, we want the detector to signal it. Statistical significance is just a tool we use to signal practical significance. Statistical significance is not meaningful in itself. For example, we do not want to have a security detector responding to a minimal quantity of metal in a person's dental filling. Statistical significance is important if it signals practical significance. We will return to this topic in Chapter 6.  

#### Unstandardized and standardized effect sizes  
The difference between our sample outcome and the hypothesized value is called _effect size_. If we test a mean, the effect size is just the difference between our sample mean and and the hypothesized population mean. For example, if we hypothesized that average candy weight in the population is 2.8 gram and we find an average candy weight in our sample bag of 2.75 gram, the effect size is 0.05 gram. If we test the difference between two means, the effect size is the difference we find in our sample minus the difference that we hypothesized, usually no difference at all, that is, zero difference.  

These effect sizes are unstandardized, which means that they depend on the scale on which we measure the sample outcome. Unstandardized effect sizes change if we measure candy weight in grams, micrograms, kilograms, or ounces. Of course, changing the scale does not affect the meaning of the effect size but the number that we are looking at is very different: 0.05 gram, 50 microgram, 0.00005 kilo, or 0.00176 (avoirdupoids) ounce. The exact number, then, does not tell us whether the effect size is large or small.  
Let us return to the example of average candy weight. Assume that the candy factory wants their candies to weight 2.8 grams per candy on average. If average weight is 2.8 grams, the factory can put ten candies in one bag to obtain a weight of one ounce. According to trade regulations, bags are allowed to be slightly heavier or lighter but there is a stict rule for the maximm difference in weight allowed. If we can express this difference as a difference in average candy weight, we have a norm for our effect size. Differences less than the norm are of no practical significance but larger differences should be noted if we do not want to violate trade regulations.  

#### Cohen's _d_  
In scientific research, we rarely have precise norms for differences that are practically significant and differences that are not. Instead, we tend to think of small and large effects as differences that are large or small in comparison to the scores that we encounter.  

If weights of candies vary a lot, we will not be very impressed by a relatively small difference between observed and expected average candy weight. In contrast, if candy weight is quite constant, a small average difference is important. For this reason, standardized effect sizes for tests on sample means divide the difference between the sample mean and the population mean by the standard deviation in the sample. Thus, we take into account the variation in scores.  

This standardized effect size for tests on means is known as _Cohen's d_. It divides the difference between sample outcome and hypothesis by the standard deviation in the sample. The sample outcome can be a single mean, for example the average weight of candies, but it can also be the difference between two means, for example, the difference between average weight of yellow candies and average weight of red candies. In the latter case, the difference is divided by a combined (_pooled_) standard deviation for yellow and red candy weight.  

The direction of an effect is not relevant to effect size. For example, we do not care whether the yellow candies are on average heavier or the red candies. For this reason, Cohen's _d_ is always positive. If you obtain a negative result, just dro the minus sign.   

Using an inventory of published results of tests on one or two means, Cohen proposed rules of thumb for standardized effect sizes:  
* 0.2: weak effect,  
* 0.5: medium effect,  
* 0.8: strong effect.  
Note that Cohen's _d_ can take values above one. These are to be considered strong effects.  

__interactive content/example: How to calculate Cohen's d from SPSS output?__  

#### Association as effect size  
Measures of association such as Pearson's product-moment correlation coefficient or Spearman's rank correlation coefficient express effect size if the null hypothesis expects no correlation in the population. If zero correlation is expected, a correlation coefficient calculated for the sample expresses the difference between what is observed (sample correlation) and what is expected (zero correlation in the population). This also aplies to the standardized regression coefficient (Beta or, according to APA6, _b_*) if the null hypothesis states that there is no effect of the predictor variable on the outcome variable in the population.  

Measures of association and standardized regression coefficients are standardized, so effect size can be interpreted using rules of thumb, e.g., __TBD__. Note, however, that this interpretation applies only if the null hypothesis states that there is no correlation or regression effect in the population. Another null hypothesis about correlations or regression coefficients would change the difference between what is observed in the sample and what is expected in the population, the latter being the null hypothesis.  

#### Effect size and sample size  
We can use standardized effect sizes to express the type of effect that we are interested in without caring about the precise size of differences. We merely have to choose whether small, medium, or large effects are of practical interest to us. Preferably, we know from previous research whether small, medium, or large effects are common in our type of research. If medium or large effects are rare, we should use a sample size that allows detecting small effects. In contrast, when large effects occur frequently, we can do with a smaller sample that may miss small effects.  

Effect size as well as test statistics reflect the difference between what we expect according to the null hypothesis and what we observe in our sample. As a consequence, effect size indicators and test statistics are related. In some cases, such as Cohen's _d_, the relation between effect size and test statistic is very simple.  

The test statisic _t_ for a _t_-test on one or two means is equal to Cohen's _d_ times the square root of sample size. Here, the only difference between the two is sample size! Sample size influences the test statistic---the larger the sample, the larger the test statistic---but it does not affect effect size. This is one reason why effect size is more interesting than test statistics and their p-values.  

Sample size is what we want to know. If we know the effect size in the sample for which we want statistically significant results and we know the critical value of the test statistic, we can figure out the minimum sample size for which the test statistic exceeds the critical value. Remember: A test is statistically significant if the test statistic exceeds the critical value, so it is in the rejection region. Enter several sample sizes in the table below to find the minimum sample size needed.  

__interactive content: interactive table with sample size as first column (user can input integer numbers), effect size Cohen's d as second column (all cells randomly fixed to 'small', 'medium', or 'strong'), t-value as calculated third column (square root of sample size times 0.2, 0.5, or 0.8), critical t-value (calculated for df = sample size minus 1) as fourth column, and p-value (calculated, p-values below .05 are printed in red) as fifth column ; after entering a sample size, the table is sorted on sample size, so the resulting table shows the progression t larger samples and smaller p-values__  

Once you have entered several sample sizes and found the minimum sample size for a statistically significant test result, explain to yourself:  
* Why calculated _t_-values increase with sample size whereas p-values decrease.  
* Why some p-values are printed in red.  
* Why the critical _t_-values decrease with sample size.  

### 5.3 Hypothetical World Versus Imaginary True World  

In the preceding paragraphs, we determined sample size using the effect size that we expect to find in our sample. We should realize, however, that we are interested in the effect size in the population. The 'true' effect size, so to speak. The effect of a new medicine or a media campaign in our sample is not important but the effect in the population. This complicates the calculation of our sample size. Instead of using the effect size in our (future!) sample, we must use the effect size in the population.  

#### Imagining a population with a small effect  
Our null hypothesis states that average candy weight in the population is 2.8 grams. Let us decide that a small effect size is practically significant. We can think now of a population that could be the true population if the effect size is small. For example, a population in which average candy weight is 2.9 grams. (Do not mind why a difference of 0.1 grams represents a small effect, that is, a standardized effect size of 0.2.)  

We do not know whether average candy weight is 2.9 grams in the true population. So we may regard this as another hypothesis. Let us call this the alternative hypothesis _H_~1~. Note that this is not an ordinary alternative hypothesis because it does not include all outcomes not covered by the null hypothesis (_H_~0~). Instead, it represents only one value, which is an important value to us because it represents a population with a small effect size.  

The interactive content below illustrates this situation. The top graph represents the sampling distribution according to our null hypothesis. This sampling distribution is derived from our hypothetical population in which there is no effect. Our null hypothesis is true for this population, so there is no difference between the hypothesized and true value, hence there is no effect. For example, average candy weight is 2.8 grams in this hypothetical population.  

The bottom graph represents the sampling distribution for an imaginary population with a small effect size. Here, the alternative hypothesis is true, for example, average candy weight is 2.9 grams, a bit higher than in the hypothetical population. By the way, average candy weights are not depicted in the graphs but you should know by now that the average in a normal distribution is situated at the top of the bell shape and that the average of a sampling distribution is equal to the populaiton average because a sample mean is an unbiased estimator.  

```{r TypeI_II_errors}
#This Shiny app illustrates Type I and Type II Error. Interaction helps to understand how significance level and power are related.  
#Source: Adapted from Tarik Gouhier, type1vs2-master, https://github.com/tgouhier/type1vs2
shinyAppDir(appDir = "type1vs2",
            options=list(width="100%", height=680))
```

Before reading on, try to make sense of the two graphs and how they relate to each other:  
1. What do the values on the horizontal axes mean?  
2. What does _t_~c~ mean?  
3. Why are the _t_-values and _t_~c~values exactly the same on both horizontal axes?  
4. What do the double-sided horizontal red arcs represent in the top graph?  
5. Why are the red double-sided horizontal arcs red in the top graph and black in the bottom graph?  
6. Why are the orange sections in the top graph labelled Type I error?  
7. What happens to the probability of Type II error (the blue section in the bottom graph) if you change the significance level with the slider?  
8. What do you think Type II error means in the bottom graph?  

#### Type I error  
We have two populations, a hypothetical population and an imaginary true population. Once we have drawn our sample, we only deal with the hypothetical population as we have done in all preceding chapters. Acting as if the null hypothesis is true, we determine how (un)likely the sample is that we have drawn. If it is very unlikely, we have a p-value below the significance level and we reject the null hypothesis. We say: If the null hypothesis was true, our sample is too unlikely, so we reject the null hypothesis.  

We may be wrong. Perhaps the null hypothesis is actually true and we were just very unfortunate to draw a sample that is very different from the population. If so, we make a Type I error. The probability that we will make this error is equal to the significance level, which is usually set to .05.  

#### The world of the researcher  
This is what we are going to do once we have the sample. Let us call this the world of the researcher. But we have not yet stepped into the world of the researcher because we are still thinking about the size of our sample. We can experiment a bit and that is what we do if we ask ourselves: What is going to happen to our statistical test if the true population from which we draw our sample has average candy weight that is a bit (small effect) higher than the weight according to our null hypothesis?  

#### The alternative world of a small effect  
If we actually sample from this imaginary true population, the bottom graph represents our true sampling distribution. It shows us the probabilities (areas under the curve) of drawing a sample with a particular minimum or maximum value for the test statistic _t_. These are the probabilities of our sample if there is a small effect in the population.  

Now that we know the true sampling distribution if there is a small effect in the population, we can foresee what is going to happen when we enter the world of the researcher. The researcher is going to use the test values of the top graph to decide on the null hypothesis. If the sample _t_-value is between, say, plus and minus two (the critical values), the researcher is not going to reject the null hypothesis.  

#### Type II error  
If there is a (small) effect in the population, the null hypothesis is not true. For example, average candy weight is not 2.8 grams, it is 2.9 grams. So the null hypothesis is not true but it is not rejected. This is a _Type II error_: not rejecting a false null hypothesis.  

The probability that we make a Type II error if there is a small effect, is expressed by the blue section in the bottom graph. It is usually denoted by the Greek letter beta ($/beta$). The blue section represents the probability of drawing a sample from this population with a small effect size that has a _t_-value that is NOT in the rejection region, so the null hypothesis is NOT rejected. See the top graph.  

#### Power of the test  
The probability of NOT making a Type II error is called the _power of the test_. It is of course equal to one minus the probability of making a Type II error, that is, 1 - $\beta$. The power of the test is represented by the orange sections in the bottom graph. They represent all sample _t_ values that make the researcher reject the null hypothesis. So a false null hypothesis is rejected and we do not make an error.  

Note that we can reject the null hypothesis in two ways: If our sample happens to have a test statistic that is much higher than expected under the null hypothesis or if it is much lower. In the example above, the imagined true population has a mean that is higher than the hypothesized population mean. The bulk of the power of the test therefore is in the right tail of the bottom graph.  

However, a little bit of power is situated in the left tail. It is so small that we usually cannot see the orange section in the left tail of the bottom graph but the displayed probability shows that it is there. This is a bit strange because one could say that we reject the null hypothesis for the wrong reason: We think our null hypothesis is too high whereas it actually is too low.  

The probability that this happens is very small and usually negligible. In the interactive content, you may encounter power values in the left tail that are so small that they have to be written in scientific notation, e.g., 1.0E-10 means 1 at the tenth decimal place: 0.0000000001. Anyway, the important thing is that we reject a false null hypothesis even if it is for the 'wrong' reason. Rejecting a false null hypothesis, our conclusion is not erroneous.  

#### Effect size, sample size, and power  
The interactive content shown below illustrates the concept of test power. It draws 1,000 samples from a population in which the true population mean is equal or larger than the population mean according to the null hypothesis. _Effect size_ reflects how much the true population mean is larger than the hypothesized mean. For each sample, the test statistic _t_ is calculated and the one-sided p-value. The 1,000 _t_-values are shown in the blue histogram and the p-values are collected in the red histogram. If the test is significant, the null hypothesis is rejected. Test power and the proportion of samples for which the null hypothesis is rejected are shown above the blue histogram.  

```{r power_H0-rejections}
#This Shiny app shows the relevance of effect size and sample size to the power of a test. 
#Source: ShinyApps-spark (niet meer gevonden op GitHub)
shinyAppDir(appDir = "ttest_simulation",
            options=list(width="100%", height=580))
```

First, familiarize yourself with the histograms by figuring out the answers to the following questions:  

1. What does the vertical black line in the blue histogram represent?  
2. Which blue bars in the histogram represent the samples with statistically significant test results, that is, samples for which the null hypothesis is rejected?  
3. What does the vertical black line in the red histogram represent?  
4. Which red bars in the histogram represent the samples with statistically significant test results, that is, samples for which the null hypothesis is rejected?  

Next, formulate for yourself how test power will change if you increase effect size or sample size. Check your expectations by adjusting the values in the interactive content.  

Finally, think about this. There is one situation in which power is meaningless. Which value should you assign to effect size or sample size to create that situation? In this situation, what does the proportion of rejected nulls indicate?  

### 5.4 Sample Size And Power  

Sample size, statistical significance, effect size, and test power are related. To determine the size of your sample, you have three buttons that you should turn simultaneously. Statistical significance is the easiest button to decide on; we usually leave the significance level at .05. We do not select a smaller value because it will reduce the power of the test (with the same sample and effect size) as you may have noticed in one of the interactive content items in the preceding secion.  

For effect size, we have to choose among a small, medium, or large effect. Previous results of research similar to our research project can help us decide whether we have to reckon with small effect sizes (need a larger sample) or not. If we have a concrete number for the (unstandardized) minimum effect size that is of practical significance, we can use that number.  

For power, the conventional rule of thumb is that we like to have at least 80% probability of rejecting a false null hypothesis. You may note that the probability of NOT rejecting a true null hypothesis is higher: .95. After all, it is one minus the probability of rejecting a true null hypothesis (Type I error), which is the significance level that we use. Power is set to a lower level because the null hypothesis is usually assumed to reflect our best knowledge about the world. From this perspective, we are keener on avoiding the error of falsely rejecting the null hypothesis (our current best knowledge) than falsely accepting it. This approach, however, is not without criticisms as we will discuss in Chapter 6. Anyway, if you want to raise the power to the same level of .95, you can do so; it will require a larger sample.  

Unfortunately, test power receives little attention in several software packages for statistical analysis. Using power and effect size to calculate the required sample size is usually not provided in the package. To calculate sample size, we need dedicated software, for example A[GPower][http://www.gpower.hhu.de/].  

```{r sample_size_and_power}
#add Shiney app to determine sample size for a specified (standardized) effect size, significance level, and test power fir some (simple) tests?
#simplify PS.shiny_master 
```

#### So how do we determine sample size?  
All in all, using effect size and test power to determine the size of the sample requires several decisions on the part of the researcher. It can be difficult to specify the effect size that we should expect or that is practically relevant. If there is little prior research comparable to our new project, we cannot reasonably specify an effect size and calculate sample size.  

In practice, researchers often try to collect as large a sample as is feasible just to be on the safe side. Of course, it is important to ensure that our sample meets the requirements of the tests that we want to specify (Section 5.1).   

Does this mean that all we have learned about effect size and test power is useless? Certainly not. First of all, we should have learned that effect size is more important than statistical significance because effect size relates to practical significance.  

Second, test power and Type II errors are important in situations in which we do not reject the null hypothesis. Then, we should calculate test power to get an impression of our confidence in the result. Is our test of sufficient power to yield significant results if there is an effect in the population?  

After having drawn the sample and executing the statistical tests, we can calculate the power of our test. We know sample size and we can pick an effect size, so we can calculate the third member of the equation, namely test power. We may pick several effect sizes, e.g., a small, medium, and large effect size, to evaluate test power for different situations. How well does or test reject the null hypothesis if the true effect is small, medium, or large? We may also use the effect size in the sample as point estimate of the effect size in the population. Test power calculated thus tells us the probability that we reject the null hypothesis for this particular effect size.  

```{r test_power}
#Use Shiney app or dedicated software to determine power of some (simple) tests?
#simplify PS.shiny_master 
```

### 5.5 Research Hypothesis as Null Hypothesis  

As noted before, the research hypothesis usually is the alternative hypothesis. We expect something to change, to be(come) different rather than be or stay the same. We expect an association to be present rather than absent.  

In this situation, rejection of the null hypothesis supports our alternative hypothesis, hence our research hypothesis. We are glad if we reject the null hypothesis. We know that we can be wrong because our null hypothesis may still be true even if the probability of drawing a sample like the one we have drawn is so small that we have to reject the null hypothesis. This is the Type I error. Fortunately, we know the probability of making this error because that is the confidence level that we have chosen, usually, five percent. We can live with this probability of making an error when we reject the null hypothesis. So we are doubly glad: we found support for our research hypothesis and we know how confident we are about this support.  

__interactive content: reuse `power_H0-rejections` and make parts of the graphs clickable ; instruct the user to click on the probability expressing the confidence for the support of our research hypothesis?__

What if our research hypothesis is our null hypothesis? For example, we have a specific idea of average candy weight in the population from previous research or from specifications by the candy factory. If we want to test whether the candies have the hypothesized average weight, our research hypothesis would specify this average weight. Specifying a particular value, the research hypothesis must be the null hypothesis.  

In this situation, we find support for our research hypothesis if we do not reject the null hypothesis. We can be wrong in not rejecting the null hypothesis. If we do not reject a null hypothesis that is actually false, we make a Type II error. The significance level is irrelevant now because the significance level is the probability of making a Type I error. But if we do not reject the null hypothesis, we can never reject a true null hypothesis (Type I error). Now, the probability of making a Type II error is important, r rather, the probability of not making this error. This is the power of the test.  

So if our research hypothesis represents the null hypothesis and our research hypothesis is supported (not rejected), we need the test power to know how confident we are about the support that we have found. Here, test power is key, not statistical significance.  

### 5.6 Sample Size and Confidence Intervals  

Is sample size relevant only for null hypothesis testing? No, of course not. Sample size determines the certainty and precision of our inferences: The larger the sample, the more certain and precise our inferences. A larger sample, then, yields narrower confidence intervals at a given confidence level or a higher confidence level for a confidence interval with a given width (precision). How precise must the estimate be and how certain do we want to be?  

If you are going to estimate a confidence interval for a parameter, you have to decide on the precision and confidence that you want to attain. Precision and confidence depend on the practical situation at hand. Imagine that you do an exit poll during elections. In an exit poll, you sample voters who have just voted and you ask them for which party or candidate they voted. It is your aim to predict the winner of the elections: Who is going to receive most votes?  

If parties or candidates have very different vote shares, it may be satisfactory to have a relatively imprecise confidence interval. Even if we estimate the vote share of a party or candidate with an interval of ten percentage points, for example, Party A is going to have forty to fifty percent of the votes with 95% confidence, we are able to pinpoint the election winner with high confidence if one party is much larger than other parties. In contrast, a very competitive election with little variation in vote shares among several leading parties or candidates requires a much more precise estimate and therefore a larger sample.  

We need not go into the details of how to calculate the sample size to obtain a confidence interval with the required precision and confidence. Suffice it to say that we have to take into account two factors: the confidence level, which is just one minus the significance level, and the effect size, for example, the minimum difference in vote shares, that we deem relevant. Test power is not relevant here because we do not use a statistical test.  

### 5.7 Take-Home Points  

* Effect size is related to practical significance.  
* Statistical significance is related to both effect size and sample size.  
* Not rejecting a false null hypothesis is a Type II error. A researcher can make this error only if the null hypothesis is not rejected.  
* The probability of making a Type II error is commonly denoted with the Greek letter beta ($\beta$).  
* The probability of NOT making a Type II error is the power of the test.  
* The power of a test tells us the probability that we reject the null hypothesis if there is an effect of a particular size in the population. The larger this probability, the more confident we are that we do not overlook this effect when we do not reject the null hypothesis.  

## 6. Critical Discussion
> Key concepts: problems with null hypothesis significance testing, nill hypothesis, design/sample-based inference versus process/model-based inference, frequentist versus Bayesian inference, 

### Criticisms of Null Hypothesis Testing

In null hypothesis testing, we totally rely on the test's p-value. If this value is below .05 or another significance level, we reject the null hypothesis and we accept it otherwise. Is this a wise thing to do? Watch the video.  

<iframe width="560" height="315" src="https://www.youtube.com/embed/ez4DgdurRPg" frameborder="0" allowfullscreen></iframe>

#### Statistical significance depends heavily on sample size  
Perhaps, our chapter on null hypothesis testing should have been titled _Am I Lucky Or Unlucky?_ instead of _Am I Right Or Am I Wrong?_. When our sample is small, the power to reject a null hypothesis is rather small, so it may occur rather often that we retain the null hypothesis even if it is wrong. There is a lot of insecurity about the population if our sample is small. So we must be lucky to draw a sample that is sufficiently at odds with the null hypothesis to reject it.

If our sample is large or very large, small differences between what we expect according to our null hypothesis can be statistically significant even if the differences are too small to be of any practical value to us. A statistically significant result need not be practically significant.  

It is, however, a common mistake to think that statistical significance is a measure of the strength or practical significance of an effect. In the video, this mistaken interpretation was expressed by the type of sound associated with a p-value: the lower the significance level of the p-value, the more joyous the sound. But this is wrong. In a large sample, even irrelevant results can be highly significant and in small samples, as demonstrated in the video, results can sometimes by highly significant and sometimes be insignificant. Never forget:  

> A statistically significant result ONLY means that the null hypothesis must be rejected.  

If we want to say something about the magnitude of the effect in the population, we should use effect size. All we have is the effect size measured in our sample and a statistical test usually telling us whether or not we should reject the null hypothesis that there is no effect size in the population.  

If the statistical test is significant, we conclude that there probably is an effect in the population. We may use the effect size in the sample as a point estimate of the population effect. This effect size should be at the core of our interpretation. Is it large, small, or perhaps tiny? It is probably better to use an interval estimate than the point estimate, to which we return below.  

If the statistical test is not significant, it is tempting to conclude that the null hypothesis is true, so there is no effect in the population and we need not interpret any effect that we find in our sample. But this is not right. Finding no proof for rejecting the null hypothesis does not proof that it is right. We may simply have been unlucky with our sample or our sample may be too small to reject the null hypothesis. It is sensible, then, to evaluate the effect found in the sample in relation to sample size and, if possible, test power. If the sample is large and/or the power to reject the null hypothesis is high if the effect in the sample resembles the true effect in the population, we may indeed ignore the effect found in the sample. But if our sample or test power is small, we should realize that we have a good chance of being unlucky with our sample. It would not be wise to ignore the effect found in the sample.  

#### Knocking down straw men (over and over again)  
There is another aspect in the practice of null hypothesis testing that is not very satisfactory. Remember that null hypothesis testing was meant to force the researcher to use previous knowledge as input to her research. The development of science requires us to expand existing knowledge but does this really happen in the practice of null hypothesis testing?  

Imagine that previous research has taught us that one additional unit of exposure to advertisements for a brand increases a person's brand awareness on average by 0.1 unit if we use well-tested stadard scales for exposure and brand awareness. If we want to use this knowledge in our own research, we would hypothesize that the regression coefficient of exposure is 0.1 in a regression model predicting brand awareness.  

Well, try to test this null hypothesis in your favourite statistics software. Can you actually tell the software that the null hypothesis about this regression coefficient is 0.1? Most likely, you can't because the software automatically tests the null hypothesis that the regression coefficient is zero in the population. This approach is so prevaent that null hypotheses equating the population valu eof interest to zero have received a special name: the _nill hypothesis_ or the _nill_ for short. So you simply cannot include previous knowledge in your test here.  

The null hypothesis that there is no association between the predictor variable and the outcome variable in the population may be interesting to reject if you really have no clue about the association. But in the example above, previous knowledge makes us expect a positive association so it is really not interesting to reject the null hypothesis of no association. The null hypothesis of no association is a _straw man_ in this example. It is unlikely to stand the test and nobody should applaud if we knock down a straw.  

Rejecting the nill time and again should make us wonder about scientific progress and our contribution to it. Is there no way to accumulate our efforts?  

### Alternatives For Null Hypothesis Significance Testing  

In the social and behavioral sciences, null hypothesis testing is still the dominant type of statistical inference. For this reason, an introductory text like the current one must discuss null hypothesis significance testing. But it should discuss it thoroughly so the problems and errors that can and are frequently being made with null hypothesis testing become clear.  

The problems with null hypothesis significance testing, however, are increasingly being recognised. Alternatives to null hypothesis significance testing have been developed and are becoming more accepted within the field. In this section, some alternatives are briefly sketched.  

#### Estimation instead of hypothesis testing  
Following up on a report commissioned by the American Psychological Association (APA) _@Cohen_, the 6^th^ edition of the _Publication Manual of the American Psychological Association_ recommends reporting and interpreting confidence intervals rather than relying solely on null hypothesis tests. Estimation, then, is becoming more important: Assessing the precision of our statements about the population rather than deciding pro or con our hypothesis about the population. This is an important step forward and it is easy to accomplish if your statistical software reports confidence intervals.  

Null hypothesis testing forces the researcher to make a binary decision: reject or not reject the null hypothesis. A preceding section argued that we should take into account our confidence, more specifically, the probability of making a Type I or Type II error, if we want to interpret the test result and that we should take into account effect size. We should strive for a more nuanced interpretation of our results than a binary decision.  

The interactive content below shows three confidence intervals for a medium, tiny, and weak effect size. The value of the null hypothesis is represented by the horizontal/vertical line. For each confidence interval, answer the following questions:  
1. Must the null hypothesis be rejected?  
2. Would you interpret the effect, and, if so, how? If not, why not?  
3. If you increase sample size, how would that change your selection of the effects that you interpret?  
4. And what if you decrease sample size?  

__interactive content: three confidence intervals depicted as line segments with the point estimate as a fat dot on a scale marked strong - medium - weak - tiny - no effect - tiny - weak - medium - strong ; the first confidence interval has a (positive) medium effect that is statistically significant, the second a tiny (positive) effect that is statistically significant, and the third a weak (negative) effect that is not statistically significant ; value of the null hypothesis represented by a horizontal line crossing the scale at no effect (so: a nill) ;  user can manipulate sample size, then the widths of all three confidence intervals are adjusted__

Using confidence intervals is a way of drawing a more nuanced conclusion. A confidence interval shows us whether or not our null hypothesis must be rejected. If the value of the null hypothesis is within the confidence interval, the null hypothesis must be rejected. At the same time, however, a confidence interval displays our insecurity about the result.  

If the confidence interval is wide, we are quite insecure about the true population value. If a wide confidence interval includes the null hypothesis near one of its borders, we reject the null hypothesis but it still is plausible that the population value is substantially larger (or substantially smaller) than the hypothesized value. We should rather report that the population value seems to be larger (smaller) than specified in the null hypothesis but that we have inconclusive evidence than reporting that there is no difference because the statistical test is not significant.  

In a similar way, a very narrow confidence interval including the null hypothesis and a very narrow confidence interval excluding but near the null hypothesis should not yield opposite conclusions because the statistical test is significant in the first but not in the second situation. After all, even for the significant situation, we know with high confidence (narrow confidence interval) that the population value is close to the hypothesized value.  

Using confidence intervals in this way, we avoid the problem that effects that are not statistically significant are not published or disregarded when previous research results are used to design new research projects. Not publishing non-significant results, either because of self-selection by the researcher or selection by journal editors and reviewers, offers a misleading view of research results.  

Effect sizes that are not statistically significant are just as helpful to determine sample size as statistically significant effect sizes. A predictor variable without statistically significant effect may have a significant effect in a new research project and should not be discarded if the potential effect size is practically significant. Moreover, combining results from several research projects helps making more precise estimates of populaiton values, which brings us to Meta-analysis.  

#### Meta-analysis and replication  
Meta-analysis offers a way to capitalize on previous knowledge. In this strategy, we collect previous research on the same topic that use the same or highly similar variables. Combining the results of these studies, one can make statements with higher precision about the population. Basically, we combine the separate samples used for each single study into a large sample, which reduces the uncertainty and allows more precise inferences about the population.  

Meta-analysis is a clear example of combining research efforts to enhance our understanding. It favors estimation over hypothesis testing because the goal is to obtain more precise estimates of population values or effects. It may also refine our theories if we can identify features of the research project, for example, @@, that systematically affect (moderate) analysis results.   

It is strongly recommended as a research strategy by G. Cumming, who coined the concept _New Statistics_. See Cumming's book, website (@), or YouTube channel (@) if you are curious.  

Another approach that includes using and building upon previous results is _replication_. If we collect new data including the variables that are central in prior research and we execute the same analyses, we _replicate_ previous research. Replication is the surest tool to check results of previous research. Checks do not necessarily serve to expose fraud and mistakes; they tell us whether prior research results still hold at a later time and perhaps in another context. Thus, we can decrease the chance that our previous results derive from an atypical sample. But replication also helps us to develop more general theories, unify theories that predict the same results, and discard theories that apply only to special situations.  

#### Bayesian inference  
A more radical way of including previous knowledge in statistical inference is _bayesian inference_. Bayesian inference regards the sample that we draw as a means to update the knowledge that we already have or think we have on the population that we investigate. Our previous knowledge is our starting point and we are not going to discard our previous knowledge if a new sample points in a different direction, as we do when we reject a null hypothesis.  

Think of bayesian inference as a process that is similar to when we try to predict the weather. If I try to predict tomorrow's wheather, I am using all my experience with the weather to make a prediction. If my prediction turns out to be more or less correct, I don't change the way I predict the weather. But if my prediction is patently wrong, I try to reconsider the way I predict the weather, for example, paying attention to new indicators of the weather.  

Bayesian inference uses another concept of probability than the type of inference presented in previous chapters, which is usually called _frequentist inference_. Bayesian inference does not assume that there is a true population value. Instead, it regards the population value as a random variable, that is, as something with a probability.  

Again, think of predicting the weather. I am not saying to myself: "Let us assume that tomorrow is a rainy day. If this is correct, what is the probability that the weather today looks like it does?" Instead, I think of the probability that it will rain tomorrow. Bayesian probabilities are much more in line with our everyday concept of probability than the dice-based probabilities of frequentist inference.  

Due to this more intuitive notion of probability, the _credible interval_, which is the bayesian equivalent of the confidence interval, means what we would like the confidence interval to mean, namely the interval within which the population value is located with the selected probability.  

Bayesian inference is intuitively appealing but it is not easy to apply and it has not yet spread widely in the social and behavioral sciences. For these reasons, we merely mention this strand of statistical inference and we refrain from giving details. Its popularity, however, is increasing, so you may come in contact with bayesian inference sooner or later.  

### What If I Do Not Have a Random Sample?  

In our approach to statistical inference, we have always assumed that we could have drawn a very large number of random samples from the same population. Thus, we would create a sampling distribution that tells us about the probability of drawing the one sample that we have actually drawn. Often, we need not draw more than one sample because we can approximate the sampling distribution with a theoretical probability distribution.  

What if I do not have a random sample? Can I still estimate confidence intervals or test null hypotheses? If you carefully read reports of scientific research, you will encounter examples of statistical inference on non-random samples or data that are not samples at all but rather represent an entire population, for example, all people visiting a particular web site. So it clearly is being done.  

We should first note that statistical inference based on a random sample is the most convincing type of inference because we exactly know the nature of the uncertainty in the data. Think of exact methods for creating a sampling distribution. If we know the distribution of candy colors in the population of all candies, we can calculate the exact probability of drawing a sample bag with, for example, 25 percent of the candies being yellow. We can calculate the probability because we understand the process of random sampling. This type of statistical inference is called _design-based inference_: the uncertainty or probabilities arise from the way we designed our data collection, namely as a random sample.

There are several justifications being made for statistical inference on data that are not random samples. One justification extends design-based inference to a theoretical population. If a researcher investigates a population instead of a sample, for example, all members of a community, she may argue that this community is representative for a wider set of communities, so it can be regarded as a random sample from an imaginary population of communities. In this case, the community members have not actually be drawn at random, so we must assume that the composition of the community did arise as a random process just like the process happening when we draw members at random from a population. Because the random process is assumed rather than actually implemented by the researcher as in design-based inference, it is easier to contest this claim and the claim requires arguments to substantiate it. What arguments can the researcher give that becoming a member of this community is a lottery like random sampling? For example, chance may play a large role in visiting a particular website. If so, the people visiting the website may be regarded as a more or less random sample from the population of all people using the internet.  

Another justification does not try to argue that the collected data represent a random sample even if the data was not drawn as a random sample. Instead, this approach argues that the outcomes that we observe for selected respondents or subjects could have been different because chance plays a role in the process from which the outcomes result. The celebrity endorsing a @ campaign can only affect the willingness to donate money through a process in which a person is exposed to this campaign, interprets the message, attaches sentiments and arguments to the message, feels morally obliged to donate, and expresses a particular willingness to donate. Probably, the process contains many more steps but the point is that chance may play a role at each unobserved step. A person may be more or less strongly exposed to the campaign, she may be more or less susceptible to this type of message, or in a better or worse mood for caring about other people, and so on. Many unoberved conditions may have independent and different effects on the final outcome during the process. A researcher can accomodate for these unobseved effects by adding a random component to the model. This is called _model-based inference_; it assumes that we can and must capture chance in a statistical model. A bell-shaped probability model such as the normal distribution is a plausible candidate for capturing the effects of many independent causes. But again, the presence and shape of the random process must be assumed and we can be wrong on that account. It is easier to contest this approach than design-based inference.  

Finally, you may have tried to draw a random sample but the actual sample that you obtain is not random due to selective non-response or problems during the sampling process. Should you use statistical inference? The key question here is: Can you describe the population from which the sample is likely to be a random sample? For example, if your survey or experiment mainly includes students, you may perhaps generalize the results to students. Design-based inference can than be applied to generalize results but the population is not as grand as you meant it to be.  

### Take-Home Points  

* Statistically significant results need not be relevant or important. A small, negligible difference between the sample outcome and the hypothesized population value can be statistically significant in a very large sample.  
* A relevant and important difference between the sample outcome and the hypothesized population value need not be statistically significant in a small sample.  


## References  
Cumming, G. (2012). Understanding the new statistics : Effect sizes, confidence intervals, and meta-analysis. New York : Routledge.
Rex Kline (2004) provides an excellent review of problems with null hypothesis testing.  
Hayes () on process (model) inference (Section @) and on mediaiton and moderation
